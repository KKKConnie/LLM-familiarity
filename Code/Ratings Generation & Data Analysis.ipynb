{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29668507-a7aa-4dad-aacb-978dab576a63",
   "metadata": {},
   "source": [
    "# Familiarity Rating Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9b7a8",
   "metadata": {},
   "source": [
    "### GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c7d25-1f2b-4587-8320-2b5ff9e3710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages\n",
    "import numpy as np\n",
    "import random\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "# Experiment Settings\n",
    "RepeatTime = 1 # number of times each cue is answered by chatGPT\n",
    "AnswerTime = 780000 # total number of responses to request\n",
    "WordOnce = 1 # number of cues sent per request\n",
    "\n",
    "# OpenAI Parameters\n",
    "MyAPI = \"sk-xxxxxx\" # OpenAI API key\n",
    "MyModel = \"gpt-4o-2024-08-06\" # model to use\n",
    "MyTemperature = 0 # sampling temperature (0-2); lower -> less diverse outputs\n",
    "MyMaxTokens = 100 # max tokens per request (includes prompt + response)\n",
    "MyFreqPenalty = 0 # frequency penalty (0-1); higher -> reduces repetition\n",
    "\n",
    "client = OpenAI(api_key = MyAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdad8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input the cue words\n",
    "with open(\"./Stimulus_27624.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    data = file.read()  # read entire file content into a string\n",
    "\n",
    "    # split by newline and remove empty lines to create the cue list\n",
    "    CUES = [i for i in data.split(\"\\n\") if i != \"\"] \n",
    "\n",
    "print(len(CUES))\n",
    "print(CUES[0:WordOnce])  # show the first few cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f898e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomize items in the CUES\n",
    "CUES = [j for i in [random.sample(CUES, len(CUES)) for i in range(RepeatTime)] for j in i] \n",
    "# Randomly shuffle CUES RepeatTime times to create a concatenated list repeated RepeatTime times\n",
    "\n",
    "CUES_all = CUES\n",
    "\n",
    "print(len(CUES_all))\n",
    "print(CUES_all[0:3]) # Show the first 3 items of CUES_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, warnings, pickle, csv\n",
    "from openai import OpenAI\n",
    "\n",
    "class Gpta:\n",
    "\n",
    "    def __init__(self, data, log, results):\n",
    "        self.log = log\n",
    "        self.results = results\n",
    "        self.data = data\n",
    "        self.cue = []\n",
    "        self.temp = MyTemperature\n",
    "        self.maxtokens = MyMaxTokens\n",
    "        self.freqpenalty = MyFreqPenalty\n",
    "        self.client = OpenAI(api_key=MyAPI)\n",
    "\n",
    "    def gpt_associations(self, cue, model=MyModel):\n",
    "        # Build the message list for the API request\n",
    "        message = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Complete the following tasks as a native speaker of Simplified Chinese: Familiarity is a measure of how familiar something is. \"\n",
    "                    + \"A Chinese word is very FAMILIAR if you see/hear it often and it is easily recognisable.\"\n",
    "                    + \"In contrast, a Chinese word is very UNFAMILIAR if you rarely see/hear it and it is relatively unrecognisable. \"\n",
    "                    + \"Please indicate how familiar you think each Chinese word is on a scale from 1 (VERY UNFAMILIAR) to 7 (VERY FAMILIAR), with the midpoint representing moderate familiarity. \"\n",
    "                    + \"The Chinese word is: \"\n",
    "                    + cue\n",
    "                    + \" Only answer a number from 1 to 7. Please limit your answer to numbers.\"\n",
    "                ),\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Send the chat completion request to the OpenAI model\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=message,\n",
    "            temperature=self.temp,\n",
    "            max_tokens=self.maxtokens,\n",
    "            frequency_penalty=self.freqpenalty,\n",
    "            logprobs=True,  # return log probabilities for output tokens\n",
    "            top_logprobs=7,  # return top logprobs for each token\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        self.cue.append(cue)\n",
    "\n",
    "        # Append cue and model response to the log file\n",
    "        with open(self.log, \"a\", encoding=\"utf-8\") as file:\n",
    "            # Write the cue\n",
    "            file.write(cue + \"，，，\\n\")\n",
    "            # Write the model's response\n",
    "            file.write(response.choices[0].message.content + \"。。。\\n\")\n",
    "\n",
    "            # Extract logprobs information for each token\n",
    "            logprobs_content = response.choices[0].logprobs.content  # get logprobs content\n",
    "\n",
    "            # Iterate tokens to format top_logprobs entries\n",
    "            top_logprobs_str = \"\"\n",
    "            for token_info in logprobs_content:\n",
    "                top_logprobs = token_info.top_logprobs\n",
    "\n",
    "                # Convert top_logprobs to simple 'token: logprob' string format\n",
    "                top_logprobs_entry = \", \".join(\n",
    "                    [f\"'{top_prob.token}': {top_prob.logprob}\" for top_prob in top_logprobs]\n",
    "                )\n",
    "                # Accumulate top_logprobs lines into a single string\n",
    "                top_logprobs_str += f\"{top_logprobs_entry}\\n\"\n",
    "\n",
    "            # Write the accumulated top_logprobs to the log file\n",
    "            file.write(top_logprobs_str)\n",
    "\n",
    "        # Append the result dict to the results list\n",
    "        self.results.append(\n",
    "            {\n",
    "                \"cue\": cue,\n",
    "                \"model\": model,\n",
    "                \"message\": message,\n",
    "                \"response\": response,\n",
    "                \"temperature\": self.temp,\n",
    "                \"max_tokens\": self.maxtokens,\n",
    "                \"frequency_penalty\": self.freqpenalty,\n",
    "                \"top_logprobs_details\": logprobs_content,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Save the current results list to a pickle file\n",
    "        with open(self.data, \"wb\") as file:\n",
    "            pickle.dump(self.results, file)\n",
    "\n",
    "        print(\"获取并保存线索成功：\" + cue)\n",
    "        print(\"chatGPT的回答是：\" + response.choices[0].message.content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpta = Gpta(\n",
    "    data = \"GPT_familiar_results_27624_expression_7.pkl\",\n",
    "    log = \"GPT_familiar_results_27624_expression_7.txt\",\n",
    "    results=[],\n",
    ")\n",
    "\n",
    "# Process unique cues and collect responses\n",
    "k = 0\n",
    "for i in CUES_all: # iterate through each cue\n",
    "    gpta.gpt_associations(i) # call API to get and save response\n",
    "    k += 1\n",
    "    if k >= AnswerTime: # stop after AnswerTime responses\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212216b",
   "metadata": {},
   "source": [
    "### Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import time, warnings, pickle, csv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "# Experiment Settings\n",
    "RepeatTime = 1 # number of times each cue is answered by Qwen\n",
    "AnswerTime = 1000000 # total number of responses to request\n",
    "WordOnce = 1 # number of cues sent per request\n",
    "\n",
    "# OpenAI Parameters\n",
    "\n",
    "MyAPI = \"sk-xxxxx\" # OpenAI API key\n",
    "MyModel = \"qwen-max\" # model to use\n",
    "MyTemperature = 0.7 # sampling temperature (0-2)\n",
    "MyMaxTokens = 100 # max tokens per request (includes prompt + response)\n",
    "\n",
    "client = OpenAI(api_key = MyAPI, base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\")  # client configured for compatible-mode endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b624fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input the cue words\n",
    "with open(\"./Stimulus_27624.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    CUES = [i for i in data.split(\"\\n\") if i != \"\"] \n",
    "\n",
    "print(len(CUES))\n",
    "print(CUES[0:WordOnce]) # show the count and the first WordOnce items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79378106",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomize items in the CUES\n",
    "CUES = [j for i in [random.sample(CUES, len(CUES)) for i in range(RepeatTime)] for j in i] \n",
    "CUES_all = CUES\n",
    "\n",
    "print(len(CUES_all))\n",
    "print(CUES_all[0:3]) # Print total count and the first 3 items of CUES_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e80c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set functions getting access to qwen\n",
    "\n",
    "class Gpta: \n",
    "\n",
    "    def __init__(self, data, log, results): \n",
    "        self.log = log\n",
    "        self.results = results\n",
    "        self.data = data\n",
    "        self.cue = [] # list to track processed cues\n",
    "        self.temp = MyTemperature  # sampling temperature (from notebook settings)\n",
    "        self.maxtokens = MyMaxTokens  # max tokens per request (from notebook settings)\n",
    "        self.client = OpenAI(api_key = MyAPI)  # OpenAI client created with API key\n",
    "\n",
    "    def gpt_associations(self, cue, model = MyModel): \n",
    "        # Send a single cue to the model and save the response.\n",
    "        # Build the user prompt (in Chinese) that asks for a familiarity rating 1-7.\n",
    "        message = [\n",
    "            {\n",
    "                \"role\": \"user\", # role: user message to the model\n",
    "                \"content\": \"作为一个简体中文母语者完成以下任务：熟悉度是衡量某个东西对你来说有多熟悉的测量标准。\"\n",
    "                + \"如果一个中文词或者汉字是你经常看到或听到的，并且很容易认出来，那么它就是非常熟悉的。\"\n",
    "                + \"相反，如果一个中文词或者汉字是你很少看到或听到的，并且不太容易认出来，那么它就是非常不熟悉的。\"\n",
    "                + \"请在1（非常不熟悉）到7（非常熟悉）的范围内，评估每个中文词或者汉字在你看来有多熟悉，其中的中点代表适中的熟悉度。\"\n",
    "                + \"这一个中文词或者汉字是：\"\n",
    "                + cue\n",
    "                + \"请只回答一个从1到7的数字，并确保答案仅限于数字。\", # user prompt content\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Call the chat completion endpoint\n",
    "        response = client.chat.completions.create(\n",
    "            model = model,\n",
    "            messages = message,\n",
    "            temperature = self.temp,\n",
    "            max_tokens = self.maxtokens,\n",
    "            stream = False,\n",
    "        )\n",
    "\n",
    "        self.cue.append(cue) # record the cue\n",
    "\n",
    "        with open(self.log, \"a\", encoding = \"utf-8\") as file: # append cue and model reply to log file\n",
    "            # Write cue and model text to the log\n",
    "            file.write(\n",
    "                cue\n",
    "                + \"，，，\\n\"\n",
    "                + response.choices[0].message.content\n",
    "                + \"。。。\"\n",
    "                + \"\\n\"\n",
    "            )\n",
    "\n",
    "        # Append a result dictionary to the in-memory results list\n",
    "        self.results.append(\n",
    "            {\n",
    "                \"cue\": cue,\n",
    "                \"model\": model,\n",
    "                \"message\": message,\n",
    "                \"response\": response,\n",
    "                \"temperature\": self.temp,\n",
    "                \"max_tokens\": self.maxtokens,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Persist results list to a pickle file\n",
    "        with open(self.data, \"wb\") as file:\n",
    "            pickle.dump(self.results, file)\n",
    "\n",
    "        # Print confirmations\n",
    "        print(\"获取并保存线索成功：\" + cue)\n",
    "        print(\"qwen的回答是：\" + response.choices[0].message.content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b428936",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpta = Gpta(\n",
    "    data = \"qwen_familiar_results.pkl\",\n",
    "    log = \"qwen_familiar_results.txt\",\n",
    "    results=[],\n",
    ")\n",
    "\n",
    "# Iterate through cues and request ratings; stop after AnswerTime\n",
    "k = 0\n",
    "for cue in CUES_all:  \n",
    "    try:\n",
    "        # Send request and count successful responses\n",
    "        gpta.gpt_associations(cue)  \n",
    "        k += 1  \n",
    "\n",
    "        if k >= AnswerTime: \n",
    "            break\n",
    "\n",
    "        # brief pause between requests to avoid rate limits (0.3s)\n",
    "        time.sleep(0.3) \n",
    "    except Exception as e:\n",
    "        # log exceptions and continue\n",
    "        print(f\"发生错误：{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa8a99",
   "metadata": {},
   "source": [
    "# Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0abc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_statistics(df, output_file):\n",
    "    # Create an ExcelWriter object for output (engine='xlsxwriter')\n",
    "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "        # Compute grouped statistics by Length and save to a separate sheet per variable\n",
    "        for column in ['GPT_FAM_probs']:\n",
    "            stats_by_length = df.groupby('Length').agg({\n",
    "                column: ['mean', 'std', 'min', 'max', 'count']\n",
    "            })\n",
    "        \n",
    "            stats_by_length.columns = ['_'.join(col).strip() for col in stats_by_length.columns.values]\n",
    "            stats_by_length.to_excel(writer, sheet_name=column)\n",
    "        \n",
    "        print(f\"统计量已保存为 {output_file}\")\n",
    "\n",
    "# Load input Excel file\n",
    "df = pd.read_excel('27624_word_7_cleaned.xlsx')\n",
    "\n",
    "# Run and save statistics to output Excel file\n",
    "calculate_statistics(df, 'unique_counts_statistics_word_GPTFAM.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcac64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired-samples t-test\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# file paths for word and expression datasets\n",
    "word_file_path = '27624_word_7_cleaned.xlsx'\n",
    "expression_file_path = '27624_expression_7_cleaned.xlsx' \n",
    "\n",
    "# Load Excel files\n",
    "word_df = pd.read_excel(word_file_path)\n",
    "expression_df = pd.read_excel(expression_file_path)\n",
    "\n",
    "# Select 'WORD', 'Length' and 'GPT_FAM_probs' from word; rename expression GPT_FAM_probs column\n",
    "word_df_selected = word_df[['WORD', 'Length', 'GPT_FAM_probs']]\n",
    "expression_df_selected = expression_df[['WORD', 'GPT_FAM_probs']].rename(columns={'GPT_FAM_probs': 'GPT_FAM_probs_expression'})\n",
    "\n",
    "# Merge on 'WORD' to keep shared items\n",
    "merged_df = pd.merge(word_df_selected, expression_df_selected, on='WORD')\n",
    "\n",
    "# Get unique Length values\n",
    "length_values = merged_df['Length'].unique()\n",
    "\n",
    "# For each Length group, perform paired t-test between GPT_FAM_probs and GPT_FAM_probs_expression\n",
    "for length in length_values:\n",
    "    group = merged_df[merged_df['Length'] == length]\n",
    "    \n",
    "    # sample size for the current group\n",
    "    sample_size = len(group)\n",
    "    \n",
    "    # compute means and standard deviations\n",
    "    mean_word = group['GPT_FAM_probs'].mean()\n",
    "    mean_expression = group['GPT_FAM_probs_expression'].mean()\n",
    "    std_word = group['GPT_FAM_probs'].std()\n",
    "    std_expression = group['GPT_FAM_probs_expression'].std()\n",
    "\n",
    "    # paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(group['GPT_FAM_probs'], group['GPT_FAM_probs_expression'])\n",
    "    \n",
    "    # degrees of freedom\n",
    "    df = len(group['GPT_FAM_probs']) - 1\n",
    "    \n",
    "    # Cohen's d for paired samples\n",
    "    diff = group['GPT_FAM_probs'] - group['GPT_FAM_probs_expression']\n",
    "    cohen_d = diff.mean() / diff.std(ddof=1)\n",
    "\n",
    "    # print formatted results\n",
    "    print(f\"Length = {length} 组内的样本量: {sample_size}\")\n",
    "    print(f\"t 统计量: {t_stat:.4f}, 自由度 (df): {df}, p 值: {p_value:.4f}\")\n",
    "    print(f\"均值 (word): {mean_word:.4f}, 标准差 (word): {std_word:.4f}\")\n",
    "    print(f\"均值 (expression): {mean_expression:.4f}, 标准差 (expression): {std_expression:.4f}\")\n",
    "    print(f\"Cohen's d (效应量): {cohen_d:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb94e0f",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa88cf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WordLength  SampleSize Pearsonr  PValue\n",
      "0           1        2336   0.6161  0.0000\n",
      "1           2           0      N/A     N/A\n",
      "2           3           0      N/A     N/A\n",
      "3           4           0      N/A     N/A\n"
     ]
    }
   ],
   "source": [
    "# Compute bivariate correlations by word length - single-character items\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_excel('27624_expression_7_cleaned.xlsx')  \n",
    "\n",
    "# Group by Length and compute Pearson correlation per group\n",
    "results = []\n",
    "for name, group in data.groupby('Length'):\n",
    "    # Drop rows with NaN or infinite values in the relevant columns\n",
    "    group = group.replace([np.inf, -np.inf], np.nan).dropna(subset=['GPT_FAM_probs','Human_FAM_Liu']) # 'SUBTLEX_logWF','SUBTLEX_logW_CD','SUBTLEX_logCHR','SUBTLEX_logCHR_CD'(当算和词频的相关的时候)\n",
    "    \n",
    "    # Compute sample size\n",
    "    sample_size = group.shape[0]\n",
    "\n",
    "    # Require at least 2 observations to compute correlation\n",
    "    if sample_size > 1:\n",
    "        # Compute Pearson r and p-value, format to 4 decimals\n",
    "        r, p_value = pearsonr(group['GPT_FAM_probs'], group['Human_FAM_Liu'])\n",
    "        results.append((name, sample_size, f\"{r:.4f}\", f\"{p_value:.4f}\"))\n",
    "    else:\n",
    "        # Not enough data for this length\n",
    "        results.append((name, sample_size, \"N/A\", \"N/A\"))\n",
    "\n",
    "# Convert results to DataFrame for display\n",
    "result_df = pd.DataFrame(results, columns=['WordLength', 'SampleSize', 'Pearsonr', 'PValue'])\n",
    "\n",
    "# Print results\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bivariate correlations by word length - multi-character items\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel('27624_word_7_cleaned.xlsx')  \n",
    "\n",
    "# Group by Length and compute correlation per group\n",
    "results = []\n",
    "for name, group in data.groupby('Length'):\n",
    "    # Remove rows with NaN or infinite values (keep relevant English tokens)\n",
    "    group = group.replace([np.inf, -np.inf], np.nan).dropna(subset=['GPT_FAM_probs', 'Human_FAM_M_Su']) # 'SUBTLEX_logWF','SUBTLEX_logW_CD'\n",
    "    \n",
    "    # Compute sample size\n",
    "    sample_size = group.shape[0]\n",
    "\n",
    "    # Require at least 2 observations to compute Pearson r\n",
    "    if sample_size > 1:\n",
    "        r, p_value = pearsonr(group['GPT_FAM_probs'], group['Human_FAM_M_Su'])\n",
    "        # Format r and p-value to 3 decimals\n",
    "        results.append((name, sample_size, f\"{r:.3f}\", f\"{p_value:.3f}\"))\n",
    "    else:\n",
    "        # Not enough data for this length\n",
    "        results.append((name, sample_size, \"N/A\", \"N/A\"))\n",
    "\n",
    "# Convert results to DataFrame for display\n",
    "result_df = pd.DataFrame(results, columns=['WordLength', 'SampleSize', 'Pearsonr', 'PValue'])\n",
    "\n",
    "# Print results\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2510c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute partial correlation controlling for Length\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "\n",
    "# Load Excel file into a DataFrame\n",
    "file_path = '27624_word_7_cleaned.xlsx' \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Drop rows missing the variables needed for the analysis\n",
    "df_cleaned = df.dropna(subset=['GPT_FAM_probs', 'Human_FAM_M_Su', 'Length'])\n",
    "\n",
    "# Compute partial correlation controlling for Length\n",
    "partial_corr_result = pg.partial_corr(data=df_cleaned, x='GPT_FAM_probs', y='Human_FAM_M_Su', covar='Length')\n",
    "\n",
    "# Print the partial correlation result; format p-value to 4 decimal places\n",
    "print(partial_corr_result.to_string(formatters={'p-val': '{:.4f}'.format}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b3d38",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c693d",
   "metadata": {},
   "source": [
    "## Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d17c55c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Length           Variable  R-squared  Coefficient  P-value  Standard Error  \\\n",
      "0       1      GPT_FAM_probs      0.431       -0.138      0.0           0.008   \n",
      "1       1   qwen_FAM_mean_30      0.394       -0.187      0.0           0.011   \n",
      "2       1      Human_FAM_Liu      0.264       -0.216      0.0           0.017   \n",
      "3       1      SUBTLEX_logWF      0.376       -0.189      0.0           0.012   \n",
      "4       1    SUBTLEX_logW_CD      0.380       -0.211      0.0           0.013   \n",
      "5       1     SUBTLEX_logCHR      0.380       -0.207      0.0           0.013   \n",
      "6       1  SUBTLEX_logCHR_CD      0.376       -0.260      0.0           0.016   \n",
      "\n",
      "   Sample Size  \n",
      "0          440  \n",
      "1          440  \n",
      "2          440  \n",
      "3          440  \n",
      "4          440  \n",
      "5          440  \n",
      "6          440  \n"
     ]
    }
   ],
   "source": [
    "# Univariate linear regression - single-character items - LDT task\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data file and specify columns to read\n",
    "file_path = '27624_expression_7_cleaned_filtered.xlsx'  # Excel file path\n",
    "y_column = 'zRT_LDT'  # dependent variable\n",
    "x_columns = [\"GPT_FAM_probs\",\"qwen_FAM_mean_30\",\"Human_FAM_Liu\",\"SUBTLEX_logWF\",\"SUBTLEX_logW_CD\",\"SUBTLEX_logCHR\",\"SUBTLEX_logCHR_CD\"]  # predictors\n",
    "length_column = 'Length'  # word length column\n",
    "\n",
    "# Read Excel and select needed columns\n",
    "data = pd.read_excel(file_path, usecols=x_columns + [y_column, length_column])\n",
    "\n",
    "# Remove rows with missing values in any predictor or the outcome\n",
    "data_cleaned = data.dropna(subset=x_columns + [y_column])\n",
    "\n",
    "# Collect regression results\n",
    "final_results = []\n",
    "\n",
    "# Loop over each Length group and run univariate OLS for each predictor\n",
    "lengths = data_cleaned[length_column].unique()\n",
    "for length in lengths:\n",
    "    length_data = data_cleaned[data_cleaned[length_column] == length]\n",
    "    \n",
    "    for x in x_columns:\n",
    "        formula = f'{y_column} ~ {x}'\n",
    "        model = smf.ols(formula, data=length_data).fit()\n",
    "        \n",
    "        final_results.append({\n",
    "            'Length': length,\n",
    "            'Variable': x,\n",
    "            'R-squared': round(model.rsquared, 3),\n",
    "            'Coefficient': round(model.params[x], 3),\n",
    "            'P-value': round(model.pvalues[x], 4),\n",
    "            'Standard Error': round(model.bse[x], 3),\n",
    "            'Sample Size': int(model.nobs)\n",
    "        })\n",
    "\n",
    "# Convert results list to DataFrame and display\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da38103f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Length           Variable  R-squared  Coefficient  P-value  Standard Error  \\\n",
      "0       1      GPT_FAM_probs      0.325       -0.397      0.0           0.012   \n",
      "1       1   qwen_FAM_mean_30      0.283       -0.542      0.0           0.018   \n",
      "2       1      Human_FAM_Liu      0.209       -0.615      0.0           0.025   \n",
      "3       1      SUBTLEX_logWF      0.289       -0.565      0.0           0.018   \n",
      "4       1    SUBTLEX_logW_CD      0.290       -0.623      0.0           0.020   \n",
      "5       1     SUBTLEX_logCHR      0.322       -0.614      0.0           0.019   \n",
      "6       1  SUBTLEX_logCHR_CD      0.330       -0.772      0.0           0.023   \n",
      "\n",
      "   Sample Size  \n",
      "0         2308  \n",
      "1         2308  \n",
      "2         2308  \n",
      "3         2308  \n",
      "4         2308  \n",
      "5         2308  \n",
      "6         2308  \n"
     ]
    }
   ],
   "source": [
    "# Univariate linear regression - single-character items - Naming task\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data\n",
    "file_path = '27624_expression_7_cleaned.xlsx'  \n",
    "y_column = 'zRT_Nam_Liu'  # Dependent variable column name\n",
    "x_columns = [\"GPT_FAM_probs\",\"qwen_FAM_mean_30\",\"Human_FAM_Liu\",\"SUBTLEX_logWF\",\"SUBTLEX_logW_CD\",\"SUBTLEX_logCHR\",\"SUBTLEX_logCHR_CD\"]  # Predictor variable names\n",
    "length_column = 'Length'  # Word length column nam\n",
    "\n",
    "# Read Excel file (select columns)\n",
    "data = pd.read_excel(file_path, usecols=x_columns + [y_column, length_column])\n",
    "\n",
    "# Drop rows with missing values in predictors or outcome\n",
    "data_cleaned = data.dropna(subset=x_columns + [y_column])\n",
    "\n",
    "# Store regression results for each length\n",
    "final_results = []\n",
    "\n",
    "# Iterate over word lengths\n",
    "lengths = data_cleaned[length_column].unique()  # Get unique lengths\n",
    "for length in lengths:\n",
    "    # Filter data for current length\n",
    "    length_data = data_cleaned[data_cleaned[length_column] == length]\n",
    "    \n",
    "    # Run univariate regression for each predictor\n",
    "    for x in x_columns:\n",
    "        # Fit OLS model\n",
    "        formula = f'{y_column} ~ {x}'\n",
    "        model = smf.ols(formula, data=length_data).fit()\n",
    "        \n",
    "        # Save metrics\n",
    "        final_results.append({\n",
    "            'Length': length,\n",
    "            'Variable': x,\n",
    "            'R-squared': round(model.rsquared, 3),\n",
    "            'Coefficient': round(model.params[x], 3),\n",
    "            'P-value': round(model.pvalues[x], 4),\n",
    "            'Standard Error': round(model.bse[x], 3),\n",
    "            'Sample Size': int(model.nobs)  # Add sample size\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Print results\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d94344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Length                Variable  R-squared  Coefficient  P-value  \\\n",
      "0        2           GPT_FAM_probs      0.377       -0.137   0.0000   \n",
      "1        2        qwen_FAM_mean_30      0.319       -0.203   0.0000   \n",
      "2        2          Human_FAM_M_Su      0.297       -0.231   0.0000   \n",
      "3        2           SUBTLEX_logWF      0.357       -0.196   0.0000   \n",
      "4        2         SUBTLEX_logW_CD      0.360       -0.212   0.0000   \n",
      "5        2      GPT_FAM_probs_head      0.061       -0.049   0.0000   \n",
      "6        2     SUBTLEX_logCHR_head      0.055       -0.076   0.0000   \n",
      "7        2  SUBTLEX_logCHR_CD_head      0.063       -0.115   0.0000   \n",
      "8        3           GPT_FAM_probs      0.335       -0.149   0.0000   \n",
      "9        3        qwen_FAM_mean_30      0.267       -0.205   0.0000   \n",
      "10       3          Human_FAM_M_Su      0.274       -0.280   0.0000   \n",
      "11       3           SUBTLEX_logWF      0.176       -0.145   0.0000   \n",
      "12       3         SUBTLEX_logW_CD      0.183       -0.159   0.0000   \n",
      "13       3      GPT_FAM_probs_head      0.015       -0.036   0.0007   \n",
      "14       3     SUBTLEX_logCHR_head      0.012       -0.051   0.0019   \n",
      "15       3  SUBTLEX_logCHR_CD_head      0.014       -0.094   0.0010   \n",
      "16       4           GPT_FAM_probs      0.186       -0.114   0.0000   \n",
      "17       4        qwen_FAM_mean_30      0.206       -0.215   0.0000   \n",
      "18       4          Human_FAM_M_Su      0.201       -0.313   0.0000   \n",
      "19       4           SUBTLEX_logWF      0.221       -0.206   0.0000   \n",
      "20       4         SUBTLEX_logW_CD      0.222       -0.212   0.0000   \n",
      "21       4      GPT_FAM_probs_head      0.021       -0.032   0.0023   \n",
      "22       4     SUBTLEX_logCHR_head      0.016       -0.044   0.0072   \n",
      "23       4  SUBTLEX_logCHR_CD_head      0.018       -0.067   0.0046   \n",
      "\n",
      "    Standard Error  Sample Size  \n",
      "0            0.002         8386  \n",
      "1            0.003         8386  \n",
      "2            0.004         8386  \n",
      "3            0.003         8386  \n",
      "4            0.003         8386  \n",
      "5            0.002         8386  \n",
      "6            0.003         8386  \n",
      "7            0.005         8386  \n",
      "8            0.008          775  \n",
      "9            0.012          775  \n",
      "10           0.016          775  \n",
      "11           0.011          775  \n",
      "12           0.012          775  \n",
      "13           0.011          775  \n",
      "14           0.016          775  \n",
      "15           0.029          775  \n",
      "16           0.011          450  \n",
      "17           0.020          450  \n",
      "18           0.029          450  \n",
      "19           0.018          450  \n",
      "20           0.019          450  \n",
      "21           0.010          450  \n",
      "22           0.016          450  \n",
      "23           0.024          450  \n"
     ]
    }
   ],
   "source": [
    "# Univariate linear regression - multi-character words - LDT task\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data\n",
    "file_path = '27624_word_7_cleaned_filtered.xlsx'  \n",
    "y_column = 'zRT_LDT' \n",
    "x_columns = [\"GPT_FAM_probs\",\"qwen_FAM_mean_30\",\"Human_FAM_M_Su\",\"SUBTLEX_logWF\",\"SUBTLEX_logW_CD\",\"GPT_FAM_probs_head\",\"SUBTLEX_logCHR_head\",\"SUBTLEX_logCHR_CD_head\"]  \n",
    "length_column = 'Length'  \n",
    "\n",
    "# Read Excel file\n",
    "data = pd.read_excel(file_path, usecols=x_columns + [y_column, length_column])\n",
    "\n",
    "# Clean dataset: keep rows with no missing values in predictors and outcome\n",
    "data_cleaned = data.dropna(subset=x_columns + [y_column])\n",
    "\n",
    "# Store regression results per word length\n",
    "final_results = []\n",
    "\n",
    "# Group by word length\n",
    "lengths = data_cleaned[length_column].unique()  \n",
    "for length in lengths:\n",
    "    # Filter data for the current length\n",
    "    length_data = data_cleaned[data_cleaned[length_column] == length]\n",
    "    \n",
    "    # Run univariate regression for each predictor\n",
    "    for x in x_columns:\n",
    "        # Fit OLS model for y ~ x\n",
    "        formula = f'{y_column} ~ {x}'\n",
    "        model = smf.ols(formula, data=length_data).fit()\n",
    "        \n",
    "        # Save regression metrics\n",
    "        final_results.append({\n",
    "            'Length': length,\n",
    "            'Variable': x,\n",
    "            'R-squared': round(model.rsquared, 3),\n",
    "            'Coefficient': round(model.params[x], 3),\n",
    "            'P-value': round(model.pvalues[x], 4),\n",
    "            'Standard Error': round(model.bse[x], 3),\n",
    "            'Sample Size': int(model.nobs)  # 添加样本量\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Print results\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51af50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Length                Variable  R-squared  Coefficient  P-value  \\\n",
      "0        2           GPT_FAM_probs      0.116       -0.039   0.0000   \n",
      "1        2        qwen_FAM_mean_30      0.102       -0.064   0.0000   \n",
      "2        2          Human_FAM_M_Su      0.041       -0.055   0.0000   \n",
      "3        2           SUBTLEX_logWF      0.110       -0.048   0.0000   \n",
      "4        2         SUBTLEX_logW_CD      0.110       -0.053   0.0000   \n",
      "5        2      GPT_FAM_probs_head      0.097       -0.035   0.0000   \n",
      "6        2     SUBTLEX_logCHR_head      0.120       -0.061   0.0000   \n",
      "7        2  SUBTLEX_logCHR_CD_head      0.112       -0.090   0.0000   \n",
      "8        3           GPT_FAM_probs      0.136       -0.039   0.0317   \n",
      "9        3        qwen_FAM_mean_30      0.165       -0.067   0.0170   \n",
      "10       3          Human_FAM_M_Su      0.112       -0.079   0.0535   \n",
      "11       3           SUBTLEX_logWF      0.051       -0.029   0.2002   \n",
      "12       3         SUBTLEX_logW_CD      0.070       -0.037   0.1299   \n",
      "13       3      GPT_FAM_probs_head      0.040       -0.022   0.2546   \n",
      "14       3     SUBTLEX_logCHR_head      0.193       -0.080   0.0093   \n",
      "15       3  SUBTLEX_logCHR_CD_head      0.305       -0.133   0.0007   \n",
      "16       4           GPT_FAM_probs      0.065       -0.030   0.0375   \n",
      "17       4        qwen_FAM_mean_30      0.074       -0.052   0.0257   \n",
      "18       4          Human_FAM_M_Su      0.149       -0.130   0.0013   \n",
      "19       4           SUBTLEX_logWF      0.050       -0.044   0.0687   \n",
      "20       4         SUBTLEX_logW_CD      0.053       -0.047   0.0600   \n",
      "21       4      GPT_FAM_probs_head      0.005       -0.009   0.5553   \n",
      "22       4     SUBTLEX_logCHR_head      0.005       -0.013   0.5683   \n",
      "23       4  SUBTLEX_logCHR_CD_head      0.011       -0.031   0.3913   \n",
      "\n",
      "    Standard Error  Sample Size  \n",
      "0            0.003         1640  \n",
      "1            0.005         1640  \n",
      "2            0.007         1640  \n",
      "3            0.003         1640  \n",
      "4            0.004         1640  \n",
      "5            0.003         1640  \n",
      "6            0.004         1640  \n",
      "7            0.006         1640  \n",
      "8            0.017           34  \n",
      "9            0.026           34  \n",
      "10           0.039           34  \n",
      "11           0.022           34  \n",
      "12           0.024           34  \n",
      "13           0.019           34  \n",
      "14           0.029           34  \n",
      "15           0.036           34  \n",
      "16           0.014           67  \n",
      "17           0.023           67  \n",
      "18           0.039           67  \n",
      "19           0.024           67  \n",
      "20           0.025           67  \n",
      "21           0.016           67  \n",
      "22           0.022           67  \n",
      "23           0.036           67  \n"
     ]
    }
   ],
   "source": [
    "# Univariate linear regression - multi-character words - Naming task -Zhang et al.,2023\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data\n",
    "file_path = '27624_word_7_cleaned_filtered_nam.xlsx'  \n",
    "y_column = 'zRT_Nam_Zhang'   \n",
    "x_columns = [\"GPT_FAM_probs\",\"qwen_FAM_mean_30\",\"Human_FAM_M_Su\",\"SUBTLEX_logWF\",\"SUBTLEX_logW_CD\",\"GPT_FAM_probs_head\",\"SUBTLEX_logCHR_head\",\"SUBTLEX_logCHR_CD_head\"]  \n",
    "length_column = 'Length'  \n",
    "\n",
    "# Read Excel file\n",
    "data = pd.read_excel(file_path, usecols=x_columns + [y_column, length_column])\n",
    "\n",
    "# Ensure rows include no missing values for all predictors and the outcome\n",
    "data_cleaned = data.dropna(subset=x_columns + [y_column])\n",
    "\n",
    "# Store regression results\n",
    "final_results = []\n",
    "\n",
    "# Iterate over word lengths and run univariate regression for each predictor\n",
    "lengths = data_cleaned[length_column].unique()  \n",
    "for length in lengths:\n",
    "    # Filter data for current length\n",
    "    length_data = data_cleaned[data_cleaned[length_column] == length]\n",
    "    \n",
    "    # Run univariate OLS for each predictor\n",
    "    for x in x_columns:\n",
    "        # Fit the model y ~ x\n",
    "        formula = f'{y_column} ~ {x}'\n",
    "        model = smf.ols(formula, data=length_data).fit()\n",
    "        \n",
    "        # Save regression metrics\n",
    "        final_results.append({\n",
    "            'Length': length,\n",
    "            'Variable': x,\n",
    "            'R-squared': round(model.rsquared, 3),\n",
    "            'Coefficient': round(model.params[x], 3),\n",
    "            'P-value': round(model.pvalues[x], 4),\n",
    "            'Standard Error': round(model.bse[x], 3),\n",
    "            'Sample Size': int(model.nobs)  \n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and print\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Print results\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e745d4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Length                Variable  R-squared  Coefficient  P-value  \\\n",
      "0       2           GPT_FAM_probs      0.193       -0.316      0.0   \n",
      "1       2        qwen_FAM_mean_30      0.203       -0.528      0.0   \n",
      "2       2          Human_FAM_M_Su      0.100       -0.384      0.0   \n",
      "3       2           SUBTLEX_logWF      0.109       -0.349      0.0   \n",
      "4       2         SUBTLEX_logW_CD      0.111       -0.375      0.0   \n",
      "5       2      GPT_FAM_probs_head      0.252       -0.336      0.0   \n",
      "6       2     SUBTLEX_logCHR_head      0.225       -0.507      0.0   \n",
      "7       2  SUBTLEX_logCHR_CD_head      0.232       -0.762      0.0   \n",
      "\n",
      "   Standard Error  Sample Size  \n",
      "0           0.015         1790  \n",
      "1           0.025         1790  \n",
      "2           0.027         1790  \n",
      "3           0.024         1790  \n",
      "4           0.025         1790  \n",
      "5           0.014         1790  \n",
      "6           0.022         1790  \n",
      "7           0.033         1790  \n"
     ]
    }
   ],
   "source": [
    "# Univariate linear regression - multi-character words - Naming task\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data\n",
    "file_path = '27624_word_7_cleaned.xlsx'  \n",
    "y_column = 'zRT_Nam_Hendrix'  # zRT_Nam_Hendrix \n",
    "x_columns = [\"GPT_FAM_probs\",\"qwen_FAM_mean_30\",\"Human_FAM_M_Su\",\"SUBTLEX_logWF\",\"SUBTLEX_logW_CD\",\"GPT_FAM_probs_head\",\"SUBTLEX_logCHR_head\",\"SUBTLEX_logCHR_CD_head\"]  \n",
    "length_column = 'Length'  \n",
    "\n",
    "# Read Excel file\n",
    "data = pd.read_excel(file_path, usecols=x_columns + [y_column, length_column])\n",
    "\n",
    "# Ensure rows include no missing values for all predictors and the outcome\n",
    "data_cleaned = data.dropna(subset=x_columns + [y_column])\n",
    "\n",
    "# Store regression results\n",
    "final_results = []\n",
    "\n",
    "# Iterate over word lengths and run univariate regression for each predictor\n",
    "lengths = data_cleaned[length_column].unique()  \n",
    "for length in lengths:\n",
    "    # Filter data for current length\n",
    "    length_data = data_cleaned[data_cleaned[length_column] == length]\n",
    "    \n",
    "    # Run univariate OLS for each predictor\n",
    "    for x in x_columns:\n",
    "        # Fit the model y ~ x\n",
    "        formula = f'{y_column} ~ {x}'\n",
    "        model = smf.ols(formula, data=length_data).fit()\n",
    "        \n",
    "        # Save regression metrics\n",
    "        final_results.append({\n",
    "            'Length': length,\n",
    "            'Variable': x,\n",
    "            'R-squared': round(model.rsquared, 3),\n",
    "            'Coefficient': round(model.params[x], 3),\n",
    "            'P-value': round(model.pvalues[x], 4),\n",
    "            'Standard Error': round(model.bse[x], 3),\n",
    "            'Sample Size': int(model.nobs)  \n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and print\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Print results\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e81568",
   "metadata": {},
   "source": [
    "## Univariate Nonlinear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最终的回归结果表:\n",
      " Length          Variable  R_squared  P_value  Sample_Size\n",
      "      1     GPT_FAM_probs      0.441      0.0          440\n",
      "      1  qwen_FAM_mean_30      0.404      0.0          440\n",
      "      1     Human_FAM_Liu      0.269      0.0          440\n",
      "      1     SUBTLEX_logWF      0.390      0.0          440\n",
      "      1   SUBTLEX_logW_CD      0.386      0.0          440\n",
      "      1    SUBTLEX_logCHR      0.404      0.0          440\n",
      "      1 SUBTLEX_logCHR_CD      0.379      0.0          440\n"
     ]
    }
   ],
   "source": [
    "# Univariate Nonlinear regression - single-character words - LDT task\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix\n",
    "\n",
    "# --------- Step 1: Read data ---------\n",
    "FILE_PATH = r\"D:/0 ECNU/CAILAB/LLM_familiarity/Data/27624_expression_7_cleaned_filtered.xlsx\"  # modify if needed\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# --------- Step 2: Define variables ---------\n",
    "y_column = 'zRT_LDT'\n",
    "x_columns = [\n",
    "    'GPT_FAM_probs',\n",
    "    'qwen_FAM_mean_30',\n",
    "    'Human_FAM_Liu',\n",
    "    'SUBTLEX_logWF',\n",
    "    'SUBTLEX_logW_CD',\n",
    "    'SUBTLEX_logCHR',\n",
    "    'SUBTLEX_logCHR_CD'\n",
    "]\n",
    "\n",
    "# Keep rows with complete cases on selected vars (+ Length)\n",
    "needed_cols = [y_column, 'Length'] + x_columns\n",
    "df_clean = df.loc[:, needed_cols].copy()\n",
    "\n",
    "# --------- Step 2.1: Ensure predictors are numeric ---------\n",
    "# Convert all selected predictors to numeric (coerce non-numeric to NaN)\n",
    "for col in x_columns + [y_column]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Drop rows with any NaNs in required columns\n",
    "df_clean = df_clean.dropna(subset=needed_cols).reset_index(drop=True)\n",
    "\n",
    "# Safety check\n",
    "non_numeric = [c for c in x_columns if not np.issubdtype(df_clean[c].dtype, np.number)]\n",
    "if len(non_numeric) > 0:\n",
    "    raise ValueError(f\"Some variables could not be converted to numeric: {', '.join(non_numeric)}\")\n",
    "\n",
    "# --------- Step 3: Unique word lengths ---------\n",
    "lengths = sorted(df_clean['Length'].unique())\n",
    "\n",
    "# --------- Prepare final results ---------\n",
    "final_results = []\n",
    "\n",
    "# --------- Step 4~8: Loop by Length and predictor; fit OLS with spline ---------\n",
    "for length_value in lengths:\n",
    "    length_data = df_clean[df_clean['Length'] == length_value].copy()\n",
    "    # Skip tiny groups\n",
    "    if len(length_data) < 5:\n",
    "        print(f\"[Skip] Length={length_value}: sample too small (n={len(length_data)})\")\n",
    "        continue\n",
    "\n",
    "    for x in x_columns:\n",
    "        # Build formula: y ~ cr(x, df=4)\n",
    "        # patsy/statsmodels support cr() via patsy; use smf.ols with formula\n",
    "        formula = f\"{y_column} ~ cr({x}, df=4)\"\n",
    "\n",
    "        try:\n",
    "            fit = smf.ols(formula, data=length_data).fit()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Length={length_value}, Var={x}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --------- Step 8: Collect stats ---------\n",
    "        final_results.append({\n",
    "            \"Length\": length_value,\n",
    "            \"Variable\": x,\n",
    "            \"R_squared\": round(fit.rsquared, 3),\n",
    "            \"P_value\": round(float(fit.f_pvalue), 4),\n",
    "            \"Sample_Size\": int(len(length_data))\n",
    "        })\n",
    "\n",
    "# --------- Step 9: Print final results table ---------\n",
    "final_df = pd.DataFrame(final_results)\n",
    "if not final_df.empty:\n",
    "    final_df[\"Variable\"] = pd.Categorical(final_df[\"Variable\"], categories=x_columns, ordered=True)\n",
    "    final_df = final_df.sort_values(by=[\"Length\", \"Variable\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n最终的回归结果表:\")\n",
    "    print(final_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo models were fitted (empty results).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "883cb9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最终的回归结果表:\n",
      " Length          Variable  R_squared  P_value  Sample_Size\n",
      "      1     GPT_FAM_probs      0.341      0.0         2308\n",
      "      1  qwen_FAM_mean_30      0.306      0.0         2308\n",
      "      1     Human_FAM_Liu      0.210      0.0         2308\n",
      "      1     SUBTLEX_logWF      0.305      0.0         2308\n",
      "      1   SUBTLEX_logW_CD      0.295      0.0         2308\n",
      "      1    SUBTLEX_logCHR      0.337      0.0         2308\n",
      "      1 SUBTLEX_logCHR_CD      0.334      0.0         2308\n"
     ]
    }
   ],
   "source": [
    "# Univariate Nonlinear regression - single-character words - Naming task\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix\n",
    "\n",
    "# --------- Step 1: Read data ---------\n",
    "FILE_PATH = r\"D:/0 ECNU/CAILAB/LLM_familiarity/Data/27624_expression_7_cleaned.xlsx\"  # modify if needed\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# --------- Step 2: Define variables ---------\n",
    "y_column = 'zRT_Nam_Liu'\n",
    "x_columns = [\n",
    "    'GPT_FAM_probs',\n",
    "    'qwen_FAM_mean_30',\n",
    "    'Human_FAM_Liu',\n",
    "    'SUBTLEX_logWF',\n",
    "    'SUBTLEX_logW_CD',\n",
    "    'SUBTLEX_logCHR',\n",
    "    'SUBTLEX_logCHR_CD'\n",
    "]\n",
    "\n",
    "# Keep rows with complete cases on selected vars (+ Length)\n",
    "needed_cols = [y_column, 'Length'] + x_columns\n",
    "df_clean = df.loc[:, needed_cols].copy()\n",
    "\n",
    "# --------- Step 2.1: Ensure predictors are numeric ---------\n",
    "# Convert all selected predictors to numeric (coerce non-numeric to NaN)\n",
    "for col in x_columns + [y_column]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Drop rows with any NaNs in required columns\n",
    "df_clean = df_clean.dropna(subset=needed_cols).reset_index(drop=True)\n",
    "\n",
    "# Safety check\n",
    "non_numeric = [c for c in x_columns if not np.issubdtype(df_clean[c].dtype, np.number)]\n",
    "if len(non_numeric) > 0:\n",
    "    raise ValueError(f\"Some variables could not be converted to numeric: {', '.join(non_numeric)}\")\n",
    "\n",
    "# --------- Step 3: Unique word lengths ---------\n",
    "lengths = sorted(df_clean['Length'].unique())\n",
    "\n",
    "# --------- Prepare final results ---------\n",
    "final_results = []\n",
    "\n",
    "# --------- Step 4~8: Loop by Length and predictor; fit OLS with spline ---------\n",
    "for length_value in lengths:\n",
    "    length_data = df_clean[df_clean['Length'] == length_value].copy()\n",
    "    # Skip tiny groups\n",
    "    if len(length_data) < 5:\n",
    "        print(f\"[Skip] Length={length_value}: sample too small (n={len(length_data)})\")\n",
    "        continue\n",
    "\n",
    "    for x in x_columns:\n",
    "        # Build formula: y ~ cr(x, df=4)\n",
    "        # patsy/statsmodels support cr() via patsy; use smf.ols with formula\n",
    "        formula = f\"{y_column} ~ cr({x}, df=4)\"\n",
    "\n",
    "        try:\n",
    "            fit = smf.ols(formula, data=length_data).fit()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Length={length_value}, Var={x}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --------- Step 8: Collect stats ---------\n",
    "        final_results.append({\n",
    "            \"Length\": length_value,\n",
    "            \"Variable\": x,\n",
    "            \"R_squared\": round(fit.rsquared, 3),\n",
    "            \"P_value\": round(float(fit.f_pvalue), 4),\n",
    "            \"Sample_Size\": int(len(length_data))\n",
    "        })\n",
    "\n",
    "# --------- Step 9: Print final results table ---------\n",
    "final_df = pd.DataFrame(final_results)\n",
    "if not final_df.empty:\n",
    "    final_df[\"Variable\"] = pd.Categorical(final_df[\"Variable\"], categories=x_columns, ordered=True)\n",
    "    final_df = final_df.sort_values(by=[\"Length\", \"Variable\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n最终的回归结果表:\")\n",
    "    print(final_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo models were fitted (empty results).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041219e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最终的回归结果表:\n",
      " Length               Variable  R_squared  P_value  Sample_Size\n",
      "      2          GPT_FAM_probs      0.379   0.0000         8386\n",
      "      2       qwen_FAM_mean_30      0.321   0.0000         8386\n",
      "      2         Human_FAM_M_Su      0.299   0.0000         8386\n",
      "      2          SUBTLEX_logWF      0.364   0.0000         8386\n",
      "      2        SUBTLEX_logW_CD      0.367   0.0000         8386\n",
      "      2     GPT_FAM_probs_head      0.063   0.0000         8386\n",
      "      2    SUBTLEX_logCHR_head      0.062   0.0000         8386\n",
      "      2 SUBTLEX_logCHR_CD_head      0.064   0.0000         8386\n",
      "      3          GPT_FAM_probs      0.338   0.0000          775\n",
      "      3       qwen_FAM_mean_30      0.269   0.0000          775\n",
      "      3         Human_FAM_M_Su      0.275   0.0000          775\n",
      "      3          SUBTLEX_logWF      0.177   0.0000          775\n",
      "      3        SUBTLEX_logW_CD      0.185   0.0000          775\n",
      "      3     GPT_FAM_probs_head      0.017   0.0037          775\n",
      "      3    SUBTLEX_logCHR_head      0.018   0.0030          775\n",
      "      3 SUBTLEX_logCHR_CD_head      0.018   0.0034          775\n",
      "      4          GPT_FAM_probs      0.202   0.0000          450\n",
      "      4       qwen_FAM_mean_30      0.213   0.0000          450\n",
      "      4         Human_FAM_M_Su      0.204   0.0000          450\n",
      "      4          SUBTLEX_logWF      0.222   0.0000          450\n",
      "      4        SUBTLEX_logW_CD      0.225   0.0000          450\n",
      "      4     GPT_FAM_probs_head      0.021   0.0258          450\n",
      "      4    SUBTLEX_logCHR_head      0.016   0.0609          450\n",
      "      4 SUBTLEX_logCHR_CD_head      0.021   0.0213          450\n"
     ]
    }
   ],
   "source": [
    "# Univariate Nonlinear regression - multi-character words - LDT task\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix\n",
    "\n",
    "# --------- Step 1: Read data ---------\n",
    "FILE_PATH = r\"D:/0 ECNU/CAILAB/LLM_familiarity/Data/27624_word_7_cleaned_filtered.xlsx\"  # modify if needed\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# --------- Step 2: Define variables ---------\n",
    "y_column = 'zRT_LDT'\n",
    "x_columns = [\n",
    "    'GPT_FAM_probs',\n",
    "    'qwen_FAM_mean_30',\n",
    "    'Human_FAM_M_Su',\n",
    "    'SUBTLEX_logWF',\n",
    "    'SUBTLEX_logW_CD',\n",
    "    'GPT_FAM_probs_head',\n",
    "    'SUBTLEX_logCHR_head',\n",
    "    'SUBTLEX_logCHR_CD_head'\n",
    "]\n",
    "\n",
    "# Keep rows with complete cases on selected vars (+ Length)\n",
    "needed_cols = [y_column, 'Length'] + x_columns\n",
    "df_clean = df.loc[:, needed_cols].copy()\n",
    "\n",
    "# --------- Step 2.1: Ensure predictors are numeric ---------\n",
    "# Convert all selected predictors to numeric (coerce non-numeric to NaN)\n",
    "for col in x_columns + [y_column]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Drop rows with any NaNs in required columns\n",
    "df_clean = df_clean.dropna(subset=needed_cols).reset_index(drop=True)\n",
    "\n",
    "# Safety check\n",
    "non_numeric = [c for c in x_columns if not np.issubdtype(df_clean[c].dtype, np.number)]\n",
    "if len(non_numeric) > 0:\n",
    "    raise ValueError(f\"Some variables could not be converted to numeric: {', '.join(non_numeric)}\")\n",
    "\n",
    "# --------- Step 3: Unique word lengths ---------\n",
    "lengths = sorted(df_clean['Length'].unique())\n",
    "\n",
    "# --------- Prepare final results ---------\n",
    "final_results = []\n",
    "\n",
    "# --------- Step 4~8: Loop by Length and predictor; fit OLS with spline ---------\n",
    "for length_value in lengths:\n",
    "    length_data = df_clean[df_clean['Length'] == length_value].copy()\n",
    "    # Skip tiny groups\n",
    "    if len(length_data) < 5:\n",
    "        print(f\"[Skip] Length={length_value}: sample too small (n={len(length_data)})\")\n",
    "        continue\n",
    "\n",
    "    for x in x_columns:\n",
    "        # Build formula: y ~ cr(x, df=4)\n",
    "        # patsy/statsmodels support cr() via patsy; use smf.ols with formula\n",
    "        formula = f\"{y_column} ~ cr({x}, df=4)\"\n",
    "\n",
    "        try:\n",
    "            fit = smf.ols(formula, data=length_data).fit()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Length={length_value}, Var={x}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --------- Step 8: Collect stats ---------\n",
    "        final_results.append({\n",
    "            \"Length\": length_value,\n",
    "            \"Variable\": x,\n",
    "            \"R_squared\": round(fit.rsquared, 3),\n",
    "            \"P_value\": round(float(fit.f_pvalue), 4),\n",
    "            \"Sample_Size\": int(len(length_data))\n",
    "        })\n",
    "\n",
    "# --------- Step 9: Print final results table ---------\n",
    "final_df = pd.DataFrame(final_results)\n",
    "if not final_df.empty:\n",
    "    final_df[\"Variable\"] = pd.Categorical(final_df[\"Variable\"], categories=x_columns, ordered=True)\n",
    "    final_df = final_df.sort_values(by=[\"Length\", \"Variable\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n最终的回归结果表:\")\n",
    "    print(final_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo models were fitted (empty results).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "438d3ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最终的回归结果表:\n",
      " Length               Variable  R_squared  P_value  Sample_Size\n",
      "      2          GPT_FAM_probs      0.120   0.0000         1640\n",
      "      2       qwen_FAM_mean_30      0.103   0.0000         1640\n",
      "      2         Human_FAM_M_Su      0.043   0.0000         1640\n",
      "      2          SUBTLEX_logWF      0.111   0.0000         1640\n",
      "      2        SUBTLEX_logW_CD      0.112   0.0000         1640\n",
      "      2     GPT_FAM_probs_head      0.098   0.0000         1640\n",
      "      2    SUBTLEX_logCHR_head      0.121   0.0000         1640\n",
      "      2 SUBTLEX_logCHR_CD_head      0.116   0.0000         1640\n",
      "      3          GPT_FAM_probs      0.166   0.1376           34\n",
      "      3       qwen_FAM_mean_30      0.252   0.0315           34\n",
      "      3         Human_FAM_M_Su      0.151   0.1712           34\n",
      "      3          SUBTLEX_logWF      0.052   0.6495           34\n",
      "      3        SUBTLEX_logW_CD      0.070   0.5275           34\n",
      "      3     GPT_FAM_probs_head      0.191   0.0919           34\n",
      "      3    SUBTLEX_logCHR_head      0.427   0.0007           34\n",
      "      3 SUBTLEX_logCHR_CD_head      0.389   0.0018           34\n",
      "      4          GPT_FAM_probs      0.107   0.0653           67\n",
      "      4       qwen_FAM_mean_30      0.081   0.1489           67\n",
      "      4         Human_FAM_M_Su      0.182   0.0051           67\n",
      "      4          SUBTLEX_logWF      0.055   0.3108           67\n",
      "      4        SUBTLEX_logW_CD      0.059   0.2758           67\n",
      "      4     GPT_FAM_probs_head      0.052   0.3365           67\n",
      "      4    SUBTLEX_logCHR_head      0.024   0.6672           67\n",
      "      4 SUBTLEX_logCHR_CD_head      0.028   0.6162           67\n"
     ]
    }
   ],
   "source": [
    "# Univariate Nonlinear regression - multi-character words - Naming task - Zhang et al.,2023\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix\n",
    "\n",
    "# --------- Step 1: Read data ---------\n",
    "FILE_PATH = r\"D:/0 ECNU/CAILAB/LLM_familiarity/Data/27624_word_7_cleaned_filtered_nam.xlsx\"  # modify if needed\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# --------- Step 2: Define variables ---------\n",
    "y_column = 'zRT_Nam_Zhang'\n",
    "x_columns = [\n",
    "    'GPT_FAM_probs',\n",
    "    'qwen_FAM_mean_30',\n",
    "    'Human_FAM_M_Su',\n",
    "    'SUBTLEX_logWF',\n",
    "    'SUBTLEX_logW_CD',\n",
    "    'GPT_FAM_probs_head',\n",
    "    'SUBTLEX_logCHR_head',\n",
    "    'SUBTLEX_logCHR_CD_head'\n",
    "]\n",
    "\n",
    "# Keep rows with complete cases on selected vars (+ Length)\n",
    "needed_cols = [y_column, 'Length'] + x_columns\n",
    "df_clean = df.loc[:, needed_cols].copy()\n",
    "\n",
    "# --------- Step 2.1: Ensure predictors are numeric ---------\n",
    "# Convert all selected predictors to numeric (coerce non-numeric to NaN)\n",
    "for col in x_columns + [y_column]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Drop rows with any NaNs in required columns\n",
    "df_clean = df_clean.dropna(subset=needed_cols).reset_index(drop=True)\n",
    "\n",
    "# Safety check\n",
    "non_numeric = [c for c in x_columns if not np.issubdtype(df_clean[c].dtype, np.number)]\n",
    "if len(non_numeric) > 0:\n",
    "    raise ValueError(f\"Some variables could not be converted to numeric: {', '.join(non_numeric)}\")\n",
    "\n",
    "# --------- Step 3: Unique word lengths ---------\n",
    "lengths = sorted(df_clean['Length'].unique())\n",
    "\n",
    "# --------- Prepare final results ---------\n",
    "final_results = []\n",
    "\n",
    "# --------- Step 4~8: Loop by Length and predictor; fit OLS with spline ---------\n",
    "for length_value in lengths:\n",
    "    length_data = df_clean[df_clean['Length'] == length_value].copy()\n",
    "    # Skip tiny groups\n",
    "    if len(length_data) < 5:\n",
    "        print(f\"[Skip] Length={length_value}: sample too small (n={len(length_data)})\")\n",
    "        continue\n",
    "\n",
    "    for x in x_columns:\n",
    "        # Build formula: y ~ cr(x, df=4)\n",
    "        # patsy/statsmodels support cr() via patsy; use smf.ols with formula\n",
    "        formula = f\"{y_column} ~ cr({x}, df=4)\"\n",
    "\n",
    "        try:\n",
    "            fit = smf.ols(formula, data=length_data).fit()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Length={length_value}, Var={x}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --------- Step 8: Collect stats ---------\n",
    "        final_results.append({\n",
    "            \"Length\": length_value,\n",
    "            \"Variable\": x,\n",
    "            \"R_squared\": round(fit.rsquared, 3),\n",
    "            \"P_value\": round(float(fit.f_pvalue), 4),\n",
    "            \"Sample_Size\": int(len(length_data))\n",
    "        })\n",
    "\n",
    "# --------- Step 9: Print final results table ---------\n",
    "final_df = pd.DataFrame(final_results)\n",
    "if not final_df.empty:\n",
    "    final_df[\"Variable\"] = pd.Categorical(final_df[\"Variable\"], categories=x_columns, ordered=True)\n",
    "    final_df = final_df.sort_values(by=[\"Length\", \"Variable\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n最终的回归结果表:\")\n",
    "    print(final_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo models were fitted (empty results).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4838f4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最终的回归结果表:\n",
      " Length               Variable  R_squared  P_value  Sample_Size\n",
      "      2          GPT_FAM_probs      0.239      0.0         1790\n",
      "      2       qwen_FAM_mean_30      0.224      0.0         1790\n",
      "      2         Human_FAM_M_Su      0.105      0.0         1790\n",
      "      2          SUBTLEX_logWF      0.110      0.0         1790\n",
      "      2        SUBTLEX_logW_CD      0.111      0.0         1790\n",
      "      2     GPT_FAM_probs_head      0.259      0.0         1790\n",
      "      2    SUBTLEX_logCHR_head      0.238      0.0         1790\n",
      "      2 SUBTLEX_logCHR_CD_head      0.233      0.0         1790\n"
     ]
    }
   ],
   "source": [
    "# Univariate Nonlinear regression - multi-character words - Naming task - Hendrix et al.,2022\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix\n",
    "\n",
    "# --------- Step 1: Read data ---------\n",
    "FILE_PATH = r\"D:/0 ECNU/CAILAB/LLM_familiarity/Data/27624_word_7_cleaned.xlsx\"  # modify if needed\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# --------- Step 2: Define variables ---------\n",
    "y_column = 'zRT_Nam_Hendrix'\n",
    "x_columns = [\n",
    "    'GPT_FAM_probs',\n",
    "    'qwen_FAM_mean_30',\n",
    "    'Human_FAM_M_Su',\n",
    "    'SUBTLEX_logWF',\n",
    "    'SUBTLEX_logW_CD',\n",
    "    'GPT_FAM_probs_head',\n",
    "    'SUBTLEX_logCHR_head',\n",
    "    'SUBTLEX_logCHR_CD_head'\n",
    "]\n",
    "\n",
    "# Keep rows with complete cases on selected vars (+ Length)\n",
    "needed_cols = [y_column, 'Length'] + x_columns\n",
    "df_clean = df.loc[:, needed_cols].copy()\n",
    "\n",
    "# --------- Step 2.1: Ensure predictors are numeric ---------\n",
    "# Convert all selected predictors to numeric (coerce non-numeric to NaN)\n",
    "for col in x_columns + [y_column]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Drop rows with any NaNs in required columns\n",
    "df_clean = df_clean.dropna(subset=needed_cols).reset_index(drop=True)\n",
    "\n",
    "# Safety check\n",
    "non_numeric = [c for c in x_columns if not np.issubdtype(df_clean[c].dtype, np.number)]\n",
    "if len(non_numeric) > 0:\n",
    "    raise ValueError(f\"Some variables could not be converted to numeric: {', '.join(non_numeric)}\")\n",
    "\n",
    "# --------- Step 3: Unique word lengths ---------\n",
    "lengths = sorted(df_clean['Length'].unique())\n",
    "\n",
    "# --------- Prepare final results ---------\n",
    "final_results = []\n",
    "\n",
    "# --------- Step 4~8: Loop by Length and predictor; fit OLS with spline ---------\n",
    "for length_value in lengths:\n",
    "    length_data = df_clean[df_clean['Length'] == length_value].copy()\n",
    "    # Skip tiny groups\n",
    "    if len(length_data) < 5:\n",
    "        print(f\"[Skip] Length={length_value}: sample too small (n={len(length_data)})\")\n",
    "        continue\n",
    "\n",
    "    for x in x_columns:\n",
    "        # Build formula: y ~ cr(x, df=4)\n",
    "        # patsy/statsmodels support cr() via patsy; use smf.ols with formula\n",
    "        formula = f\"{y_column} ~ cr({x}, df=4)\"\n",
    "\n",
    "        try:\n",
    "            fit = smf.ols(formula, data=length_data).fit()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Length={length_value}, Var={x}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --------- Step 8: Collect stats ---------\n",
    "        final_results.append({\n",
    "            \"Length\": length_value,\n",
    "            \"Variable\": x,\n",
    "            \"R_squared\": round(fit.rsquared, 3),\n",
    "            \"P_value\": round(float(fit.f_pvalue), 4),\n",
    "            \"Sample_Size\": int(len(length_data))\n",
    "        })\n",
    "\n",
    "# --------- Step 9: Print final results table ---------\n",
    "final_df = pd.DataFrame(final_results)\n",
    "if not final_df.empty:\n",
    "    final_df[\"Variable\"] = pd.Categorical(final_df[\"Variable\"], categories=x_columns, ordered=True)\n",
    "    final_df = final_df.sort_values(by=[\"Length\", \"Variable\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n最终的回归结果表:\")\n",
    "    print(final_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo models were fitted (empty results).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4855e",
   "metadata": {},
   "source": [
    "## Hierarchical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ebc6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效样本数: N = 436\n",
      "\n",
      "===== Step1 =====\n",
      "R² = 0.430 | ΔR² = 0.430\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                zRT_LDT   R-squared:                       0.430\n",
      "Model:                            OLS   Adj. R-squared:                  0.424\n",
      "Method:                 Least Squares   F-statistic:                     64.94\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):           2.04e-50\n",
      "Time:                        21:43:27   Log-Likelihood:                -496.03\n",
      "No. Observations:                 436   AIC:                             1004.\n",
      "Df Residuals:                     430   BIC:                             1029.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const              -4.51e-17      0.036  -1.24e-15      1.000      -0.072       0.072\n",
      "NoS                   0.1071      0.039      2.715      0.007       0.030       0.185\n",
      "SUBTLEX_logCHR_CD    -0.3568      0.063     -5.675      0.000      -0.480      -0.233\n",
      "NoWF                 -0.3133      0.066     -4.758      0.000      -0.443      -0.184\n",
      "NoM                   0.0539      0.045      1.200      0.231      -0.034       0.142\n",
      "NoP                  -0.0006      0.038     -0.017      0.987      -0.076       0.075\n",
      "==============================================================================\n",
      "Omnibus:                       10.573   Durbin-Watson:                   2.019\n",
      "Prob(Omnibus):                  0.005   Jarque-Bera (JB):               10.721\n",
      "Skew:                           0.380   Prob(JB):                      0.00470\n",
      "Kurtosis:                       3.114   Cond. No.                         3.67\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step2 =====\n",
      "R² = 0.502 | ΔR² = 0.072\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                zRT_LDT   R-squared:                       0.502\n",
      "Model:                            OLS   Adj. R-squared:                  0.495\n",
      "Method:                 Least Squares   F-statistic:                     72.08\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):           6.69e-62\n",
      "Time:                        21:43:27   Log-Likelihood:                -466.67\n",
      "No. Observations:                 436   AIC:                             947.3\n",
      "Df Residuals:                     429   BIC:                             975.9\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const              -4.51e-17      0.034  -1.32e-15      1.000      -0.067       0.067\n",
      "NoS                   0.0329      0.038      0.863      0.389      -0.042       0.108\n",
      "SUBTLEX_logCHR_CD    -0.2391      0.061     -3.938      0.000      -0.358      -0.120\n",
      "NoWF                 -0.1839      0.064     -2.884      0.004      -0.309      -0.059\n",
      "NoM                   0.0155      0.042      0.366      0.714      -0.068       0.099\n",
      "NoP                   0.0302      0.036      0.837      0.403      -0.041       0.101\n",
      "AoA_Liu               0.3705      0.047      7.864      0.000       0.278       0.463\n",
      "==============================================================================\n",
      "Omnibus:                       12.618   Durbin-Watson:                   2.006\n",
      "Prob(Omnibus):                  0.002   Jarque-Bera (JB):               12.898\n",
      "Skew:                           0.410   Prob(JB):                      0.00158\n",
      "Kurtosis:                       3.196   Cond. No.                         4.03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step3 =====\n",
      "R² = 0.528 | ΔR² = 0.026\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                zRT_LDT   R-squared:                       0.528\n",
      "Model:                            OLS   Adj. R-squared:                  0.521\n",
      "Method:                 Least Squares   F-statistic:                     68.47\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):           6.30e-66\n",
      "Time:                        21:43:27   Log-Likelihood:                -454.87\n",
      "No. Observations:                 436   AIC:                             925.7\n",
      "Df Residuals:                     428   BIC:                             958.4\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const              -4.51e-17      0.033  -1.36e-15      1.000      -0.065       0.065\n",
      "NoS                   0.0475      0.037      1.274      0.203      -0.026       0.121\n",
      "SUBTLEX_logCHR_CD    -0.2128      0.059     -3.581      0.000      -0.330      -0.096\n",
      "NoWF                 -0.1716      0.062     -2.758      0.006      -0.294      -0.049\n",
      "NoM                  -0.0256      0.042     -0.609      0.543      -0.108       0.057\n",
      "NoP                   0.0351      0.035      0.999      0.318      -0.034       0.104\n",
      "AoA_Liu               0.2607      0.051      5.099      0.000       0.160       0.361\n",
      "Human_FAM_Liu_log    -0.2030      0.042     -4.878      0.000      -0.285      -0.121\n",
      "==============================================================================\n",
      "Omnibus:                       17.010   Durbin-Watson:                   2.011\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               17.897\n",
      "Skew:                           0.476   Prob(JB):                     0.000130\n",
      "Kurtosis:                       3.281   Cond. No.                         4.20\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step4 =====\n",
      "R² = 0.564 | ΔR² = 0.035\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                zRT_LDT   R-squared:                       0.564\n",
      "Model:                            OLS   Adj. R-squared:                  0.555\n",
      "Method:                 Least Squares   F-statistic:                     68.94\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):           3.91e-72\n",
      "Time:                        21:43:27   Log-Likelihood:                -437.88\n",
      "No. Observations:                 436   AIC:                             893.8\n",
      "Df Residuals:                     427   BIC:                             930.5\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const              -4.51e-17      0.032  -1.41e-15      1.000      -0.063       0.063\n",
      "NoS                   0.0715      0.036      1.981      0.048       0.001       0.142\n",
      "SUBTLEX_logCHR_CD    -0.1255      0.059     -2.122      0.034      -0.242      -0.009\n",
      "NoWF                 -0.1490      0.060     -2.483      0.013      -0.267      -0.031\n",
      "NoM                  -0.0215      0.041     -0.531      0.596      -0.101       0.058\n",
      "NoP                   0.0202      0.034      0.595      0.552      -0.047       0.087\n",
      "AoA_Liu               0.1324      0.054      2.458      0.014       0.027       0.238\n",
      "Human_FAM_Liu_log    -0.1687      0.040     -4.166      0.000      -0.248      -0.089\n",
      "GPT_FAM_probs_log    -0.2935      0.050     -5.883      0.000      -0.392      -0.195\n",
      "==============================================================================\n",
      "Omnibus:                       17.883   Durbin-Watson:                   1.916\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               19.005\n",
      "Skew:                           0.477   Prob(JB):                     7.47e-05\n",
      "Kurtosis:                       3.368   Cond. No.                         4.62\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "📊 分层回归结果汇总：\n",
      "     Step           Variable   Beta     SE      t       p     R2    ΔR2\n",
      "0   Step1                NoS  0.107  0.039  2.715  0.0069  0.430  0.430\n",
      "1   Step1  SUBTLEX_logCHR_CD -0.357  0.063 -5.675  0.0000  0.430  0.430\n",
      "2   Step1               NoWF -0.313  0.066 -4.758  0.0000  0.430  0.430\n",
      "3   Step1                NoM  0.054  0.045  1.200  0.2309  0.430  0.430\n",
      "4   Step1                NoP -0.001  0.038 -0.017  0.9867  0.430  0.430\n",
      "5   Step2                NoS  0.033  0.038  0.863  0.3886  0.502  0.072\n",
      "6   Step2  SUBTLEX_logCHR_CD -0.239  0.061 -3.938  0.0001  0.502  0.072\n",
      "7   Step2               NoWF -0.184  0.064 -2.884  0.0041  0.502  0.072\n",
      "8   Step2                NoM  0.016  0.042  0.366  0.7142  0.502  0.072\n",
      "9   Step2                NoP  0.030  0.036  0.837  0.4028  0.502  0.072\n",
      "10  Step2            AoA_Liu  0.371  0.047  7.864  0.0000  0.502  0.072\n",
      "11  Step3                NoS  0.047  0.037  1.274  0.2034  0.528  0.026\n",
      "12  Step3  SUBTLEX_logCHR_CD -0.213  0.059 -3.581  0.0004  0.528  0.026\n",
      "13  Step3               NoWF -0.172  0.062 -2.758  0.0061  0.528  0.026\n",
      "14  Step3                NoM -0.026  0.042 -0.609  0.5429  0.528  0.026\n",
      "15  Step3                NoP  0.035  0.035  0.999  0.3182  0.528  0.026\n",
      "16  Step3            AoA_Liu  0.261  0.051  5.099  0.0000  0.528  0.026\n",
      "17  Step3  Human_FAM_Liu_log -0.203  0.042 -4.878  0.0000  0.528  0.026\n",
      "18  Step4                NoS  0.072  0.036  1.981  0.0483  0.564  0.035\n",
      "19  Step4  SUBTLEX_logCHR_CD -0.125  0.059 -2.122  0.0344  0.564  0.035\n",
      "20  Step4               NoWF -0.149  0.060 -2.483  0.0134  0.564  0.035\n",
      "21  Step4                NoM -0.022  0.041 -0.531  0.5959  0.564  0.035\n",
      "22  Step4                NoP  0.020  0.034  0.595  0.5524  0.564  0.035\n",
      "23  Step4            AoA_Liu  0.132  0.054  2.458  0.0144  0.564  0.035\n",
      "24  Step4  Human_FAM_Liu_log -0.169  0.040 -4.166  0.0000  0.564  0.035\n",
      "25  Step4  GPT_FAM_probs_log -0.294  0.050 -5.883  0.0000  0.564  0.035\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical regression for LDT (single-character)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ====== 1. Load dataset ======\n",
    "file_path = \"27624_expression_7_cleaned_filtered.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Define dependent variable and hierarchical predictors\n",
    "target_col = \"zRT_LDT\"\n",
    "steps = {\n",
    "    \"Step1\": [\"NoS\", \"SUBTLEX_logCHR_CD\", \"NoWF\", \"NoM\", \"NoP\"],   # Basic lexical features\n",
    "    \"Step2\": [\"AoA_Liu\"],                                           # Age of Acquisition\n",
    "    \"Step3\": [\"Human_FAM_Liu_log\"],                                 # Human familiarity\n",
    "    \"Step4\": [\"GPT_FAM_probs_log\"]                                  # GPT familiarity\n",
    "}\n",
    "\n",
    "# ====== 2. Data cleaning ======\n",
    "# Keep only the necessary columns\n",
    "cols_needed = [target_col] + [v for lst in steps.values() for v in lst]\n",
    "df = df[cols_needed].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna()\n",
    "print(f\"Valid sample size: N = {len(df)}\")\n",
    "\n",
    "# ====== 3. Standardization (for standardized β coefficients) ======\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "y = df_scaled[target_col]\n",
    "results = []\n",
    "prev_r2 = 0\n",
    "\n",
    "# ====== 4. Hierarchical regression loop ======\n",
    "for i, (step_name, vars_list) in enumerate(steps.items(), start=1):\n",
    "    # Include all variables up to the current step\n",
    "    all_vars = [v for step in list(steps.values())[:i] for v in step]\n",
    "    X = df_scaled[all_vars]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    r2 = model.rsquared\n",
    "    delta_r2 = r2 - prev_r2 if i > 1 else r2\n",
    "    prev_r2 = r2\n",
    "    \n",
    "    print(f\"\\n===== {step_name} =====\")\n",
    "    print(f\"R² = {r2:.3f} | ΔR² = {delta_r2:.3f}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Store summary results\n",
    "    for var in X.columns:\n",
    "        if var == \"const\":\n",
    "            continue\n",
    "        results.append({\n",
    "            \"Step\": step_name,\n",
    "            \"Variable\": var,\n",
    "            \"Beta\": round(model.params[var], 3),\n",
    "            \"SE\": round(model.bse[var], 3),\n",
    "            \"t\": round(model.tvalues[var], 3),\n",
    "            \"p\": round(model.pvalues[var], 4),\n",
    "            \"R2\": round(r2, 3),\n",
    "            \"ΔR2\": round(delta_r2, 3)\n",
    "        })\n",
    "\n",
    "# ====== 5. Combine and export results ======\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\n📊 Hierarchical Regression Summary:\")\n",
    "print(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5d98054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid sample size: N = 478\n",
      "\n",
      "===== Step1 =====\n",
      "R² = 0.360 | ΔR² = 0.360\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            zRT_Nam_Liu   R-squared:                       0.360\n",
      "Model:                            OLS   Adj. R-squared:                  0.353\n",
      "Method:                 Least Squares   F-statistic:                     53.06\n",
      "Date:                Mon, 27 Oct 2025   Prob (F-statistic):           1.16e-43\n",
      "Time:                        14:50:48   Log-Likelihood:                -571.66\n",
      "No. Observations:                 478   AIC:                             1155.\n",
      "Df Residuals:                     472   BIC:                             1180.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const             -1.301e-17      0.037  -3.53e-16      1.000      -0.072       0.072\n",
      "NoS                   0.0962      0.040      2.417      0.016       0.018       0.174\n",
      "SUBTLEX_logCHR_CD    -0.3553      0.067     -5.296      0.000      -0.487      -0.223\n",
      "NoWF                 -0.2560      0.071     -3.610      0.000      -0.395      -0.117\n",
      "NoM                   0.0481      0.046      1.043      0.298      -0.042       0.139\n",
      "NoP                   0.0318      0.039      0.818      0.414      -0.045       0.108\n",
      "==============================================================================\n",
      "Omnibus:                      113.892   Durbin-Watson:                   1.662\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              313.806\n",
      "Skew:                           1.147   Prob(JB):                     7.21e-69\n",
      "Kurtosis:                       6.239   Cond. No.                         3.94\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step2 =====\n",
      "R² = 0.423 | ΔR² = 0.064\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            zRT_Nam_Liu   R-squared:                       0.423\n",
      "Model:                            OLS   Adj. R-squared:                  0.416\n",
      "Method:                 Least Squares   F-statistic:                     57.66\n",
      "Date:                Mon, 27 Oct 2025   Prob (F-statistic):           2.39e-53\n",
      "Time:                        14:50:48   Log-Likelihood:                -546.62\n",
      "No. Observations:                 478   AIC:                             1107.\n",
      "Df Residuals:                     471   BIC:                             1136.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const             -1.301e-17      0.035  -3.72e-16      1.000      -0.069       0.069\n",
      "NoS                   0.0201      0.039      0.513      0.608      -0.057       0.097\n",
      "SUBTLEX_logCHR_CD    -0.2127      0.067     -3.187      0.002      -0.344      -0.082\n",
      "NoWF                 -0.1354      0.069     -1.951      0.052      -0.272       0.001\n",
      "NoM                   0.0090      0.044      0.204      0.839      -0.078       0.096\n",
      "NoP                   0.0569      0.037      1.537      0.125      -0.016       0.130\n",
      "AoA_Liu               0.3701      0.051      7.212      0.000       0.269       0.471\n",
      "==============================================================================\n",
      "Omnibus:                      115.390   Durbin-Watson:                   1.616\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              348.776\n",
      "Skew:                           1.126   Prob(JB):                     1.84e-76\n",
      "Kurtosis:                       6.526   Cond. No.                         4.36\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step3 =====\n",
      "R² = 0.430 | ΔR² = 0.006\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            zRT_Nam_Liu   R-squared:                       0.430\n",
      "Model:                            OLS   Adj. R-squared:                  0.421\n",
      "Method:                 Least Squares   F-statistic:                     50.64\n",
      "Date:                Mon, 27 Oct 2025   Prob (F-statistic):           1.41e-53\n",
      "Time:                        14:50:48   Log-Likelihood:                -543.94\n",
      "No. Observations:                 478   AIC:                             1104.\n",
      "Df Residuals:                     470   BIC:                             1137.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const             -1.301e-17      0.035  -3.74e-16      1.000      -0.068       0.068\n",
      "NoS                   0.0296      0.039      0.754      0.451      -0.048       0.107\n",
      "SUBTLEX_logCHR_CD    -0.1872      0.067     -2.780      0.006      -0.320      -0.055\n",
      "NoWF                 -0.1279      0.069     -1.849      0.065      -0.264       0.008\n",
      "NoM                  -0.0134      0.045     -0.299      0.765      -0.102       0.075\n",
      "NoP                   0.0577      0.037      1.565      0.118      -0.015       0.130\n",
      "AoA_Liu               0.3137      0.057      5.538      0.000       0.202       0.425\n",
      "Human_FAM_Liu_log    -0.1077      0.047     -2.304      0.022      -0.200      -0.016\n",
      "==============================================================================\n",
      "Omnibus:                       99.577   Durbin-Watson:                   1.613\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              259.682\n",
      "Skew:                           1.024   Prob(JB):                     4.08e-57\n",
      "Kurtosis:                       5.974   Cond. No.                         4.61\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step4 =====\n",
      "R² = 0.454 | ΔR² = 0.024\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            zRT_Nam_Liu   R-squared:                       0.454\n",
      "Model:                            OLS   Adj. R-squared:                  0.445\n",
      "Method:                 Least Squares   F-statistic:                     48.81\n",
      "Date:                Mon, 27 Oct 2025   Prob (F-statistic):           4.30e-57\n",
      "Time:                        14:50:48   Log-Likelihood:                -533.48\n",
      "No. Observations:                 478   AIC:                             1085.\n",
      "Df Residuals:                     469   BIC:                             1122.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const             -1.301e-17      0.034  -3.81e-16      1.000      -0.067       0.067\n",
      "NoS                   0.0491      0.039      1.268      0.206      -0.027       0.125\n",
      "SUBTLEX_logCHR_CD    -0.1047      0.068     -1.531      0.126      -0.239       0.030\n",
      "NoWF                 -0.1018      0.068     -1.498      0.135      -0.235       0.032\n",
      "NoM                  -0.0111      0.044     -0.252      0.801      -0.098       0.075\n",
      "NoP                   0.0422      0.036      1.164      0.245      -0.029       0.114\n",
      "AoA_Liu               0.2073      0.060      3.447      0.001       0.089       0.326\n",
      "Human_FAM_Liu_log    -0.0656      0.047     -1.405      0.161      -0.157       0.026\n",
      "GPT_FAM_probs_log    -0.2671      0.058     -4.580      0.000      -0.382      -0.153\n",
      "==============================================================================\n",
      "Omnibus:                       93.411   Durbin-Watson:                   1.569\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              246.042\n",
      "Skew:                           0.959   Prob(JB):                     3.74e-54\n",
      "Kurtosis:                       5.946   Cond. No.                         5.09\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "📊 Hierarchical Regression Summary:\n",
      "     Step           Variable   Beta     SE      t       p     R2    ΔR2\n",
      "0   Step1                NoS  0.096  0.040  2.417  0.0160  0.360  0.360\n",
      "1   Step1  SUBTLEX_logCHR_CD -0.355  0.067 -5.296  0.0000  0.360  0.360\n",
      "2   Step1               NoWF -0.256  0.071 -3.610  0.0003  0.360  0.360\n",
      "3   Step1                NoM  0.048  0.046  1.043  0.2975  0.360  0.360\n",
      "4   Step1                NoP  0.032  0.039  0.818  0.4139  0.360  0.360\n",
      "5   Step2                NoS  0.020  0.039  0.513  0.6079  0.423  0.064\n",
      "6   Step2  SUBTLEX_logCHR_CD -0.213  0.067 -3.187  0.0015  0.423  0.064\n",
      "7   Step2               NoWF -0.135  0.069 -1.951  0.0516  0.423  0.064\n",
      "8   Step2                NoM  0.009  0.044  0.204  0.8386  0.423  0.064\n",
      "9   Step2                NoP  0.057  0.037  1.537  0.1250  0.423  0.064\n",
      "10  Step2            AoA_Liu  0.370  0.051  7.212  0.0000  0.423  0.064\n",
      "11  Step3                NoS  0.030  0.039  0.754  0.4513  0.430  0.006\n",
      "12  Step3  SUBTLEX_logCHR_CD -0.187  0.067 -2.780  0.0056  0.430  0.006\n",
      "13  Step3               NoWF -0.128  0.069 -1.849  0.0650  0.430  0.006\n",
      "14  Step3                NoM -0.013  0.045 -0.299  0.7651  0.430  0.006\n",
      "15  Step3                NoP  0.058  0.037  1.565  0.1183  0.430  0.006\n",
      "16  Step3            AoA_Liu  0.314  0.057  5.538  0.0000  0.430  0.006\n",
      "17  Step3  Human_FAM_Liu_log -0.108  0.047 -2.304  0.0216  0.430  0.006\n",
      "18  Step4                NoS  0.049  0.039  1.268  0.2056  0.454  0.024\n",
      "19  Step4  SUBTLEX_logCHR_CD -0.105  0.068 -1.531  0.1264  0.454  0.024\n",
      "20  Step4               NoWF -0.102  0.068 -1.498  0.1347  0.454  0.024\n",
      "21  Step4                NoM -0.011  0.044 -0.252  0.8014  0.454  0.024\n",
      "22  Step4                NoP  0.042  0.036  1.164  0.2450  0.454  0.024\n",
      "23  Step4            AoA_Liu  0.207  0.060  3.447  0.0006  0.454  0.024\n",
      "24  Step4  Human_FAM_Liu_log -0.066  0.047 -1.405  0.1606  0.454  0.024\n",
      "25  Step4  GPT_FAM_probs_log -0.267  0.058 -4.580  0.0000  0.454  0.024\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical regression for Naming (single-character)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ====== 1. Load dataset ======\n",
    "file_path = \"27624_expression_7_cleaned.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Define dependent variable and hierarchical predictors\n",
    "target_col = \"zRT_Nam_Liu\"\n",
    "steps = {\n",
    "    \"Step1\": [\"NoS\", \"SUBTLEX_logCHR_CD\", \"NoWF\", \"NoM\", \"NoP\"],   # Basic lexical features\n",
    "    \"Step2\": [\"AoA_Liu\"],                                           # Age of Acquisition\n",
    "    \"Step3\": [\"Human_FAM_Liu_log\"],                                 # Human familiarity\n",
    "    \"Step4\": [\"GPT_FAM_probs_log\"]                                  # GPT familiarity\n",
    "}\n",
    "\n",
    "# ====== 2. Data cleaning ======\n",
    "# Keep only the necessary columns\n",
    "cols_needed = [target_col] + [v for lst in steps.values() for v in lst]\n",
    "df = df[cols_needed].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna()\n",
    "print(f\"Valid sample size: N = {len(df)}\")\n",
    "\n",
    "# ====== 3. Standardization (for standardized β coefficients) ======\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "y = df_scaled[target_col]\n",
    "results = []\n",
    "prev_r2 = 0\n",
    "\n",
    "# ====== 4. Hierarchical regression loop ======\n",
    "for i, (step_name, vars_list) in enumerate(steps.items(), start=1):\n",
    "    # Include all variables up to the current step\n",
    "    all_vars = [v for step in list(steps.values())[:i] for v in step]\n",
    "    X = df_scaled[all_vars]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    r2 = model.rsquared\n",
    "    delta_r2 = r2 - prev_r2 if i > 1 else r2\n",
    "    prev_r2 = r2\n",
    "    \n",
    "    print(f\"\\n===== {step_name} =====\")\n",
    "    print(f\"R² = {r2:.3f} | ΔR² = {delta_r2:.3f}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Store summary results\n",
    "    for var in X.columns:\n",
    "        if var == \"const\":\n",
    "            continue\n",
    "        results.append({\n",
    "            \"Step\": step_name,\n",
    "            \"Variable\": var,\n",
    "            \"Beta\": round(model.params[var], 3),\n",
    "            \"SE\": round(model.bse[var], 3),\n",
    "            \"t\": round(model.tvalues[var], 3),\n",
    "            \"p\": round(model.pvalues[var], 4),\n",
    "            \"R2\": round(r2, 3),\n",
    "            \"ΔR2\": round(delta_r2, 3)\n",
    "        })\n",
    "\n",
    "# ====== 5. Combine and export results ======\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\n📊 Hierarchical Regression Summary:\")\n",
    "print(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aebc5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid sample size: N = 9546\n",
      "\n",
      "===== Step1 =====\n",
      "R² = 0.361 | ΔR² = 0.361\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                zRT_LDT   R-squared:                       0.361\n",
      "Model:                            OLS   Adj. R-squared:                  0.360\n",
      "Method:                 Least Squares   F-statistic:                     769.2\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):               0.00\n",
      "Time:                        22:16:15   Log-Likelihood:                -11409.\n",
      "No. Observations:                9546   AIC:                         2.283e+04\n",
      "Df Residuals:                    9538   BIC:                         2.289e+04\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "const           -1.683e-16      0.008  -2.06e-14      1.000      -0.016       0.016\n",
      "Length              0.0275      0.009      3.124      0.002       0.010       0.045\n",
      "NoS                 0.0856      0.009      9.261      0.000       0.067       0.104\n",
      "CF                  0.0840      0.015      5.740      0.000       0.055       0.113\n",
      "SUBTLEX_logW_CD    -0.5848      0.009    -65.628      0.000      -0.602      -0.567\n",
      "NoWF               -0.1250      0.015     -8.136      0.000      -0.155      -0.095\n",
      "NoM                 0.0245      0.011      2.213      0.027       0.003       0.046\n",
      "NoP                 0.0251      0.009      2.845      0.004       0.008       0.042\n",
      "==============================================================================\n",
      "Omnibus:                      698.656   Durbin-Watson:                   1.875\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              995.824\n",
      "Skew:                           0.614   Prob(JB):                    5.75e-217\n",
      "Kurtosis:                       3.997   Cond. No.                         3.98\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step2 =====\n",
      "R² = 0.388 | ΔR² = 0.027\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                zRT_LDT   R-squared:                       0.388\n",
      "Model:                            OLS   Adj. R-squared:                  0.388\n",
      "Method:                 Least Squares   F-statistic:                     756.2\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):               0.00\n",
      "Time:                        22:16:15   Log-Likelihood:                -11201.\n",
      "No. Observations:                9546   AIC:                         2.242e+04\n",
      "Df Residuals:                    9537   BIC:                         2.248e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "const           -1.683e-16      0.008   -2.1e-14      1.000      -0.016       0.016\n",
      "Length              0.0066      0.009      0.762      0.446      -0.010       0.024\n",
      "NoS                 0.0779      0.009      8.609      0.000       0.060       0.096\n",
      "CF                  0.0756      0.014      5.279      0.000       0.048       0.104\n",
      "SUBTLEX_logW_CD    -0.5324      0.009    -58.617      0.000      -0.550      -0.515\n",
      "NoWF               -0.1074      0.015     -7.134      0.000      -0.137      -0.078\n",
      "NoM                 0.0139      0.011      1.283      0.200      -0.007       0.035\n",
      "NoP                 0.0252      0.009      2.918      0.004       0.008       0.042\n",
      "AoA                 0.1764      0.009     20.630      0.000       0.160       0.193\n",
      "==============================================================================\n",
      "Omnibus:                      641.728   Durbin-Watson:                   1.858\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              877.928\n",
      "Skew:                           0.593   Prob(JB):                    2.29e-191\n",
      "Kurtosis:                       3.895   Cond. No.                         3.99\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step3 =====\n",
      "R² = 0.446 | ΔR² = 0.058\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                zRT_LDT   R-squared:                       0.446\n",
      "Model:                            OLS   Adj. R-squared:                  0.446\n",
      "Method:                 Least Squares   F-statistic:                     853.4\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):               0.00\n",
      "Time:                        22:16:16   Log-Likelihood:                -10725.\n",
      "No. Observations:                9546   AIC:                         2.147e+04\n",
      "Df Residuals:                    9536   BIC:                         2.154e+04\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "const              -1.683e-16      0.008  -2.21e-14      1.000      -0.015       0.015\n",
      "Length                 0.0034      0.008      0.415      0.678      -0.013       0.020\n",
      "NoS                    0.0810      0.009      9.410      0.000       0.064       0.098\n",
      "CF                     0.0682      0.014      5.002      0.000       0.041       0.095\n",
      "SUBTLEX_logW_CD       -0.4126      0.009    -43.723      0.000      -0.431      -0.394\n",
      "NoWF                  -0.1053      0.014     -7.353      0.000      -0.133      -0.077\n",
      "NoM                    0.0284      0.010      2.742      0.006       0.008       0.049\n",
      "NoP                    0.0197      0.008      2.393      0.017       0.004       0.036\n",
      "AoA                    0.0911      0.009     10.625      0.000       0.074       0.108\n",
      "Human_FAM_M_Su_log    -0.2943      0.009    -31.601      0.000      -0.313      -0.276\n",
      "==============================================================================\n",
      "Omnibus:                      612.171   Durbin-Watson:                   1.872\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              837.838\n",
      "Skew:                           0.572   Prob(JB):                    1.16e-182\n",
      "Kurtosis:                       3.892   Cond. No.                         4.02\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step4 =====\n",
      "R² = 0.481 | ΔR² = 0.035\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                zRT_LDT   R-squared:                       0.481\n",
      "Model:                            OLS   Adj. R-squared:                  0.480\n",
      "Method:                 Least Squares   F-statistic:                     883.5\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):               0.00\n",
      "Time:                        22:16:16   Log-Likelihood:                -10415.\n",
      "No. Observations:                9546   AIC:                         2.085e+04\n",
      "Df Residuals:                    9535   BIC:                         2.093e+04\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "const              -1.683e-16      0.007  -2.28e-14      1.000      -0.014       0.014\n",
      "Length                 0.0365      0.008      4.504      0.000       0.021       0.052\n",
      "NoS                    0.0738      0.008      8.851      0.000       0.057       0.090\n",
      "CF                     0.0576      0.013      4.362      0.000       0.032       0.083\n",
      "SUBTLEX_logW_CD       -0.3094      0.010    -30.921      0.000      -0.329      -0.290\n",
      "NoWF                  -0.0453      0.014     -3.219      0.001      -0.073      -0.018\n",
      "NoM                    0.0292      0.010      2.911      0.004       0.010       0.049\n",
      "NoP                    0.0193      0.008      2.430      0.015       0.004       0.035\n",
      "AoA                    0.0462      0.008      5.440      0.000       0.030       0.063\n",
      "Human_FAM_M_Su_log    -0.2081      0.010    -21.585      0.000      -0.227      -0.189\n",
      "GPT_FAM_probs_log     -0.2711      0.011    -25.295      0.000      -0.292      -0.250\n",
      "==============================================================================\n",
      "Omnibus:                      709.041   Durbin-Watson:                   1.887\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1044.478\n",
      "Skew:                           0.608   Prob(JB):                    1.56e-227\n",
      "Kurtosis:                       4.070   Cond. No.                         4.30\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step5 =====\n",
      "R² = 0.481 | ΔR² = 0.000\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                zRT_LDT   R-squared:                       0.481\n",
      "Model:                            OLS   Adj. R-squared:                  0.480\n",
      "Method:                 Least Squares   F-statistic:                     803.3\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):               0.00\n",
      "Time:                        22:16:16   Log-Likelihood:                -10415.\n",
      "No. Observations:                9546   AIC:                         2.085e+04\n",
      "Df Residuals:                    9534   BIC:                         2.094e+04\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                  -1.683e-16      0.007  -2.28e-14      1.000      -0.014       0.014\n",
      "Length                     0.0365      0.008      4.503      0.000       0.021       0.052\n",
      "NoS                        0.0746      0.008      8.911      0.000       0.058       0.091\n",
      "CF                         0.0561      0.013      4.229      0.000       0.030       0.082\n",
      "SUBTLEX_logW_CD           -0.3087      0.010    -30.786      0.000      -0.328      -0.289\n",
      "NoWF                      -0.0472      0.014     -3.330      0.001      -0.075      -0.019\n",
      "NoM                        0.0280      0.010      2.783      0.005       0.008       0.048\n",
      "NoP                        0.0195      0.008      2.455      0.014       0.004       0.035\n",
      "AoA                        0.0461      0.008      5.436      0.000       0.030       0.063\n",
      "Human_FAM_M_Su_log        -0.2084      0.010    -21.608      0.000      -0.227      -0.190\n",
      "GPT_FAM_probs_log         -0.2742      0.011    -24.699      0.000      -0.296      -0.252\n",
      "GPT_FAM_probs_head_log     0.0099      0.009      1.073      0.283      -0.008       0.028\n",
      "==============================================================================\n",
      "Omnibus:                      709.898   Durbin-Watson:                   1.887\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1046.604\n",
      "Skew:                           0.609   Prob(JB):                    5.41e-228\n",
      "Kurtosis:                       4.072   Cond. No.                         4.57\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "📊 Hierarchical Regression Summary:\n",
      "     Step                Variable   Beta     SE       t       p     R2    ΔR2\n",
      "0   Step1                  Length  0.028  0.009   3.124  0.0018  0.361  0.361\n",
      "1   Step1                     NoS  0.086  0.009   9.261  0.0000  0.361  0.361\n",
      "2   Step1                      CF  0.084  0.015   5.740  0.0000  0.361  0.361\n",
      "3   Step1         SUBTLEX_logW_CD -0.585  0.009 -65.628  0.0000  0.361  0.361\n",
      "4   Step1                    NoWF -0.125  0.015  -8.136  0.0000  0.361  0.361\n",
      "5   Step1                     NoM  0.025  0.011   2.213  0.0269  0.361  0.361\n",
      "6   Step1                     NoP  0.025  0.009   2.845  0.0045  0.361  0.361\n",
      "7   Step2                  Length  0.007  0.009   0.762  0.4459  0.388  0.027\n",
      "8   Step2                     NoS  0.078  0.009   8.609  0.0000  0.388  0.027\n",
      "9   Step2                      CF  0.076  0.014   5.279  0.0000  0.388  0.027\n",
      "10  Step2         SUBTLEX_logW_CD -0.532  0.009 -58.617  0.0000  0.388  0.027\n",
      "11  Step2                    NoWF -0.107  0.015  -7.134  0.0000  0.388  0.027\n",
      "12  Step2                     NoM  0.014  0.011   1.283  0.1995  0.388  0.027\n",
      "13  Step2                     NoP  0.025  0.009   2.918  0.0035  0.388  0.027\n",
      "14  Step2                     AoA  0.176  0.009  20.630  0.0000  0.388  0.027\n",
      "15  Step3                  Length  0.003  0.008   0.415  0.6782  0.446  0.058\n",
      "16  Step3                     NoS  0.081  0.009   9.410  0.0000  0.446  0.058\n",
      "17  Step3                      CF  0.068  0.014   5.002  0.0000  0.446  0.058\n",
      "18  Step3         SUBTLEX_logW_CD -0.413  0.009 -43.723  0.0000  0.446  0.058\n",
      "19  Step3                    NoWF -0.105  0.014  -7.353  0.0000  0.446  0.058\n",
      "20  Step3                     NoM  0.028  0.010   2.742  0.0061  0.446  0.058\n",
      "21  Step3                     NoP  0.020  0.008   2.393  0.0167  0.446  0.058\n",
      "22  Step3                     AoA  0.091  0.009  10.625  0.0000  0.446  0.058\n",
      "23  Step3      Human_FAM_M_Su_log -0.294  0.009 -31.601  0.0000  0.446  0.058\n",
      "24  Step4                  Length  0.037  0.008   4.504  0.0000  0.481  0.035\n",
      "25  Step4                     NoS  0.074  0.008   8.851  0.0000  0.481  0.035\n",
      "26  Step4                      CF  0.058  0.013   4.362  0.0000  0.481  0.035\n",
      "27  Step4         SUBTLEX_logW_CD -0.309  0.010 -30.921  0.0000  0.481  0.035\n",
      "28  Step4                    NoWF -0.045  0.014  -3.219  0.0013  0.481  0.035\n",
      "29  Step4                     NoM  0.029  0.010   2.911  0.0036  0.481  0.035\n",
      "30  Step4                     NoP  0.019  0.008   2.430  0.0151  0.481  0.035\n",
      "31  Step4                     AoA  0.046  0.008   5.440  0.0000  0.481  0.035\n",
      "32  Step4      Human_FAM_M_Su_log -0.208  0.010 -21.585  0.0000  0.481  0.035\n",
      "33  Step4       GPT_FAM_probs_log -0.271  0.011 -25.295  0.0000  0.481  0.035\n",
      "34  Step5                  Length  0.036  0.008   4.503  0.0000  0.481  0.000\n",
      "35  Step5                     NoS  0.075  0.008   8.911  0.0000  0.481  0.000\n",
      "36  Step5                      CF  0.056  0.013   4.229  0.0000  0.481  0.000\n",
      "37  Step5         SUBTLEX_logW_CD -0.309  0.010 -30.786  0.0000  0.481  0.000\n",
      "38  Step5                    NoWF -0.047  0.014  -3.330  0.0009  0.481  0.000\n",
      "39  Step5                     NoM  0.028  0.010   2.783  0.0054  0.481  0.000\n",
      "40  Step5                     NoP  0.020  0.008   2.455  0.0141  0.481  0.000\n",
      "41  Step5                     AoA  0.046  0.008   5.436  0.0000  0.481  0.000\n",
      "42  Step5      Human_FAM_M_Su_log -0.208  0.010 -21.608  0.0000  0.481  0.000\n",
      "43  Step5       GPT_FAM_probs_log -0.274  0.011 -24.699  0.0000  0.481  0.000\n",
      "44  Step5  GPT_FAM_probs_head_log  0.010  0.009   1.073  0.2832  0.481  0.000\n"
     ]
    }
   ],
   "source": [
    "# # Hierarchical regression for LDT (multi-character)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ====== 1. Load dataset ======\n",
    "file_path = \"27624_word_7_cleaned_filtered.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Define dependent variable and hierarchical predictors\n",
    "target_col = \"zRT_LDT\"\n",
    "steps = {\n",
    "    \"Step1\": [\"Length\",\"NoS\", \"CF\",\"SUBTLEX_logW_CD\", \"NoWF\", \"NoM\", \"NoP\"],   # Basic lexical features\n",
    "    \"Step2\": [\"AoA\"],                                           # Age of Acquisition\n",
    "    \"Step3\": [\"Human_FAM_M_Su_log\"],                                 # Human familiarity\n",
    "    \"Step4\": [\"GPT_FAM_probs_log\"],                                  # GPT familiarity\n",
    "    \"Step5\": [\"GPT_FAM_probs_head_log\"]                                  # GPT head familiarity\n",
    "}\n",
    "\n",
    "# ====== 2. Data cleaning ======\n",
    "# Keep only the necessary columns\n",
    "cols_needed = [target_col] + [v for lst in steps.values() for v in lst]\n",
    "df = df[cols_needed].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna()\n",
    "print(f\"Valid sample size: N = {len(df)}\")\n",
    "\n",
    "# ====== 3. Standardization (for standardized β coefficients) ======\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "y = df_scaled[target_col]\n",
    "results = []\n",
    "prev_r2 = 0\n",
    "\n",
    "# ====== 4. Hierarchical regression loop ======\n",
    "for i, (step_name, vars_list) in enumerate(steps.items(), start=1):\n",
    "    # Include all variables up to the current step\n",
    "    all_vars = [v for step in list(steps.values())[:i] for v in step]\n",
    "    X = df_scaled[all_vars]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    r2 = model.rsquared\n",
    "    delta_r2 = r2 - prev_r2 if i > 1 else r2\n",
    "    prev_r2 = r2\n",
    "    \n",
    "    print(f\"\\n===== {step_name} =====\")\n",
    "    print(f\"R² = {r2:.3f} | ΔR² = {delta_r2:.3f}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Store summary results\n",
    "    for var in X.columns:\n",
    "        if var == \"const\":\n",
    "            continue\n",
    "        results.append({\n",
    "            \"Step\": step_name,\n",
    "            \"Variable\": var,\n",
    "            \"Beta\": round(model.params[var], 3),\n",
    "            \"SE\": round(model.bse[var], 3),\n",
    "            \"t\": round(model.tvalues[var], 3),\n",
    "            \"p\": round(model.pvalues[var], 4),\n",
    "            \"R2\": round(r2, 3),\n",
    "            \"ΔR2\": round(delta_r2, 3)\n",
    "        })\n",
    "\n",
    "# ====== 5. Combine and export results ======\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\n📊 Hierarchical Regression Summary:\")\n",
    "print(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17d3f7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid sample size: N = 1096\n",
      "\n",
      "===== Step1 =====\n",
      "R² = 0.161 | ΔR² = 0.161\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          zRT_Nam_Zhang   R-squared:                       0.161\n",
      "Model:                            OLS   Adj. R-squared:                  0.155\n",
      "Method:                 Least Squares   F-statistic:                     29.80\n",
      "Date:                Mon, 27 Oct 2025   Prob (F-statistic):           8.08e-38\n",
      "Time:                        14:51:38   Log-Likelihood:                -1459.0\n",
      "No. Observations:                1096   AIC:                             2934.\n",
      "Df Residuals:                    1088   BIC:                             2974.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "const           -6.765e-17      0.028  -2.44e-15      1.000      -0.054       0.054\n",
      "Length              0.1173      0.029      4.055      0.000       0.061       0.174\n",
      "NoS                 0.0340      0.031      1.111      0.267      -0.026       0.094\n",
      "CF                 -0.1239      0.049     -2.528      0.012      -0.220      -0.028\n",
      "SUBTLEX_logW_CD    -0.2656      0.032     -8.401      0.000      -0.328      -0.204\n",
      "NoWF               -0.0476      0.050     -0.944      0.345      -0.147       0.051\n",
      "NoM                -0.0211      0.038     -0.559      0.576      -0.095       0.053\n",
      "NoP                 0.0048      0.030      0.163      0.870      -0.053       0.063\n",
      "==============================================================================\n",
      "Omnibus:                      114.278   Durbin-Watson:                   1.463\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              165.384\n",
      "Skew:                           0.772   Prob(JB):                     1.22e-36\n",
      "Kurtosis:                       4.113   Cond. No.                         3.88\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step2 =====\n",
      "R² = 0.161 | ΔR² = 0.000\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          zRT_Nam_Zhang   R-squared:                       0.161\n",
      "Model:                            OLS   Adj. R-squared:                  0.155\n",
      "Method:                 Least Squares   F-statistic:                     26.08\n",
      "Date:                Mon, 27 Oct 2025   Prob (F-statistic):           4.22e-37\n",
      "Time:                        14:51:38   Log-Likelihood:                -1458.9\n",
      "No. Observations:                1096   AIC:                             2936.\n",
      "Df Residuals:                    1087   BIC:                             2981.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "const           -6.765e-17      0.028  -2.44e-15      1.000      -0.055       0.055\n",
      "Length              0.1162      0.029      3.999      0.000       0.059       0.173\n",
      "NoS                 0.0341      0.031      1.114      0.265      -0.026       0.094\n",
      "CF                 -0.1250      0.049     -2.546      0.011      -0.221      -0.029\n",
      "SUBTLEX_logW_CD    -0.2626      0.032     -8.094      0.000      -0.326      -0.199\n",
      "NoWF               -0.0454      0.051     -0.894      0.372      -0.145       0.054\n",
      "NoM                -0.0223      0.038     -0.589      0.556      -0.097       0.052\n",
      "NoP                 0.0045      0.030      0.151      0.880      -0.054       0.063\n",
      "AoA                 0.0122      0.029      0.420      0.675      -0.045       0.069\n",
      "==============================================================================\n",
      "Omnibus:                      114.357   Durbin-Watson:                   1.460\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              165.476\n",
      "Skew:                           0.772   Prob(JB):                     1.17e-36\n",
      "Kurtosis:                       4.113   Cond. No.                         3.92\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step3 =====\n",
      "R² = 0.172 | ΔR² = 0.011\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          zRT_Nam_Zhang   R-squared:                       0.172\n",
      "Model:                            OLS   Adj. R-squared:                  0.165\n",
      "Method:                 Least Squares   F-statistic:                     25.10\n",
      "Date:                Mon, 27 Oct 2025   Prob (F-statistic):           1.92e-39\n",
      "Time:                        14:51:38   Log-Likelihood:                -1451.6\n",
      "No. Observations:                1096   AIC:                             2923.\n",
      "Df Residuals:                    1086   BIC:                             2973.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "const              -6.765e-17      0.028  -2.45e-15      1.000      -0.054       0.054\n",
      "Length                 0.1055      0.029      3.635      0.000       0.049       0.162\n",
      "NoS                    0.0376      0.030      1.233      0.218      -0.022       0.097\n",
      "CF                    -0.1223      0.049     -2.506      0.012      -0.218      -0.027\n",
      "SUBTLEX_logW_CD       -0.2193      0.034     -6.418      0.000      -0.286      -0.152\n",
      "NoWF                  -0.0476      0.050     -0.944      0.345      -0.147       0.051\n",
      "NoM                   -0.0168      0.038     -0.445      0.656      -0.091       0.057\n",
      "NoP                    0.0007      0.029      0.024      0.981      -0.057       0.058\n",
      "AoA                   -0.0146      0.030     -0.492      0.623      -0.073       0.044\n",
      "Human_FAM_M_Su_log    -0.1218      0.032     -3.832      0.000      -0.184      -0.059\n",
      "==============================================================================\n",
      "Omnibus:                      111.643   Durbin-Watson:                   1.469\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              159.026\n",
      "Skew:                           0.765   Prob(JB):                     2.94e-35\n",
      "Kurtosis:                       4.068   Cond. No.                         3.96\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step4 =====\n",
      "R² = 0.185 | ΔR² = 0.013\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          zRT_Nam_Zhang   R-squared:                       0.185\n",
      "Model:                            OLS   Adj. R-squared:                  0.177\n",
      "Method:                 Least Squares   F-statistic:                     24.59\n",
      "Date:                Mon, 27 Oct 2025   Prob (F-statistic):           3.27e-42\n",
      "Time:                        14:51:38   Log-Likelihood:                -1443.2\n",
      "No. Observations:                1096   AIC:                             2908.\n",
      "Df Residuals:                    1085   BIC:                             2963.\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "const              -6.765e-17      0.027  -2.47e-15      1.000      -0.054       0.054\n",
      "Length                 0.1184      0.029      4.084      0.000       0.062       0.175\n",
      "NoS                    0.0312      0.030      1.030      0.303      -0.028       0.091\n",
      "CF                    -0.1337      0.049     -2.757      0.006      -0.229      -0.039\n",
      "SUBTLEX_logW_CD       -0.1271      0.041     -3.119      0.002      -0.207      -0.047\n",
      "NoWF                  -0.0104      0.051     -0.205      0.838      -0.110       0.089\n",
      "NoM                   -0.0148      0.037     -0.394      0.694      -0.088       0.059\n",
      "NoP                    0.0037      0.029      0.127      0.899      -0.054       0.061\n",
      "AoA                   -0.0540      0.031     -1.743      0.082      -0.115       0.007\n",
      "Human_FAM_M_Su_log    -0.0885      0.033     -2.716      0.007      -0.153      -0.025\n",
      "GPT_FAM_probs_log     -0.1753      0.043     -4.089      0.000      -0.259      -0.091\n",
      "==============================================================================\n",
      "Omnibus:                      100.067   Durbin-Watson:                   1.477\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              137.561\n",
      "Skew:                           0.718   Prob(JB):                     1.35e-30\n",
      "Kurtosis:                       3.974   Cond. No.                         4.40\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "===== Step5 =====\n",
      "R² = 0.205 | ΔR² = 0.020\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          zRT_Nam_Zhang   R-squared:                       0.205\n",
      "Model:                            OLS   Adj. R-squared:                  0.197\n",
      "Method:                 Least Squares   F-statistic:                     25.36\n",
      "Date:                Mon, 27 Oct 2025   Prob (F-statistic):           3.95e-47\n",
      "Time:                        14:51:38   Log-Likelihood:                -1429.6\n",
      "No. Observations:                1096   AIC:                             2883.\n",
      "Df Residuals:                    1084   BIC:                             2943.\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                  -6.765e-17      0.027   -2.5e-15      1.000      -0.053       0.053\n",
      "Length                     0.1201      0.029      4.194      0.000       0.064       0.176\n",
      "NoS                        0.0195      0.030      0.651      0.515      -0.039       0.078\n",
      "CF                        -0.1213      0.048     -2.528      0.012      -0.216      -0.027\n",
      "SUBTLEX_logW_CD           -0.1325      0.040     -3.290      0.001      -0.211      -0.053\n",
      "NoWF                       0.0355      0.051      0.695      0.487      -0.065       0.136\n",
      "NoM                        0.0025      0.037      0.066      0.947      -0.070       0.075\n",
      "NoP                        0.0038      0.029      0.132      0.895      -0.053       0.060\n",
      "AoA                       -0.0396      0.031     -1.288      0.198      -0.100       0.021\n",
      "Human_FAM_M_Su_log        -0.0787      0.032     -2.438      0.015      -0.142      -0.015\n",
      "GPT_FAM_probs_log         -0.1079      0.044     -2.437      0.015      -0.195      -0.021\n",
      "GPT_FAM_probs_head_log    -0.1811      0.035     -5.211      0.000      -0.249      -0.113\n",
      "==============================================================================\n",
      "Omnibus:                      101.856   Durbin-Watson:                   1.461\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              141.797\n",
      "Skew:                           0.722   Prob(JB):                     1.62e-31\n",
      "Kurtosis:                       4.011   Cond. No.                         4.68\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "📊 Hierarchical Regression Summary:\n",
      "     Step                Variable   Beta     SE      t       p     R2    ΔR2\n",
      "0   Step1                  Length  0.117  0.029  4.055  0.0001  0.161  0.161\n",
      "1   Step1                     NoS  0.034  0.031  1.111  0.2669  0.161  0.161\n",
      "2   Step1                      CF -0.124  0.049 -2.528  0.0116  0.161  0.161\n",
      "3   Step1         SUBTLEX_logW_CD -0.266  0.032 -8.401  0.0000  0.161  0.161\n",
      "4   Step1                    NoWF -0.048  0.050 -0.944  0.3452  0.161  0.161\n",
      "5   Step1                     NoM -0.021  0.038 -0.559  0.5761  0.161  0.161\n",
      "6   Step1                     NoP  0.005  0.030  0.163  0.8702  0.161  0.161\n",
      "7   Step2                  Length  0.116  0.029  3.999  0.0001  0.161  0.000\n",
      "8   Step2                     NoS  0.034  0.031  1.114  0.2655  0.161  0.000\n",
      "9   Step2                      CF -0.125  0.049 -2.546  0.0110  0.161  0.000\n",
      "10  Step2         SUBTLEX_logW_CD -0.263  0.032 -8.094  0.0000  0.161  0.000\n",
      "11  Step2                    NoWF -0.045  0.051 -0.894  0.3717  0.161  0.000\n",
      "12  Step2                     NoM -0.022  0.038 -0.589  0.5560  0.161  0.000\n",
      "13  Step2                     NoP  0.004  0.030  0.151  0.8800  0.161  0.000\n",
      "14  Step2                     AoA  0.012  0.029  0.420  0.6745  0.161  0.000\n",
      "15  Step3                  Length  0.105  0.029  3.635  0.0003  0.172  0.011\n",
      "16  Step3                     NoS  0.038  0.030  1.233  0.2178  0.172  0.011\n",
      "17  Step3                      CF -0.122  0.049 -2.506  0.0123  0.172  0.011\n",
      "18  Step3         SUBTLEX_logW_CD -0.219  0.034 -6.418  0.0000  0.172  0.011\n",
      "19  Step3                    NoWF -0.048  0.050 -0.944  0.3454  0.172  0.011\n",
      "20  Step3                     NoM -0.017  0.038 -0.445  0.6563  0.172  0.011\n",
      "21  Step3                     NoP  0.001  0.029  0.024  0.9811  0.172  0.011\n",
      "22  Step3                     AoA -0.015  0.030 -0.492  0.6226  0.172  0.011\n",
      "23  Step3      Human_FAM_M_Su_log -0.122  0.032 -3.832  0.0001  0.172  0.011\n",
      "24  Step4                  Length  0.118  0.029  4.084  0.0000  0.185  0.013\n",
      "25  Step4                     NoS  0.031  0.030  1.030  0.3033  0.185  0.013\n",
      "26  Step4                      CF -0.134  0.049 -2.757  0.0059  0.185  0.013\n",
      "27  Step4         SUBTLEX_logW_CD -0.127  0.041 -3.119  0.0019  0.185  0.013\n",
      "28  Step4                    NoWF -0.010  0.051 -0.205  0.8378  0.185  0.013\n",
      "29  Step4                     NoM -0.015  0.037 -0.394  0.6936  0.185  0.013\n",
      "30  Step4                     NoP  0.004  0.029  0.127  0.8990  0.185  0.013\n",
      "31  Step4                     AoA -0.054  0.031 -1.743  0.0816  0.185  0.013\n",
      "32  Step4      Human_FAM_M_Su_log -0.089  0.033 -2.716  0.0067  0.185  0.013\n",
      "33  Step4       GPT_FAM_probs_log -0.175  0.043 -4.089  0.0000  0.185  0.013\n",
      "34  Step5                  Length  0.120  0.029  4.194  0.0000  0.205  0.020\n",
      "35  Step5                     NoS  0.020  0.030  0.651  0.5151  0.205  0.020\n",
      "36  Step5                      CF -0.121  0.048 -2.528  0.0116  0.205  0.020\n",
      "37  Step5         SUBTLEX_logW_CD -0.132  0.040 -3.290  0.0010  0.205  0.020\n",
      "38  Step5                    NoWF  0.035  0.051  0.695  0.4873  0.205  0.020\n",
      "39  Step5                     NoM  0.002  0.037  0.066  0.9473  0.205  0.020\n",
      "40  Step5                     NoP  0.004  0.029  0.132  0.8948  0.205  0.020\n",
      "41  Step5                     AoA -0.040  0.031 -1.288  0.1982  0.205  0.020\n",
      "42  Step5      Human_FAM_M_Su_log -0.079  0.032 -2.438  0.0149  0.205  0.020\n",
      "43  Step5       GPT_FAM_probs_log -0.108  0.044 -2.437  0.0150  0.205  0.020\n",
      "44  Step5  GPT_FAM_probs_head_log -0.181  0.035 -5.211  0.0000  0.205  0.020\n"
     ]
    }
   ],
   "source": [
    "# # Hierarchical regression for Naming (multi-character)(Zhang et al.,2023)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ====== 1. Load dataset ======\n",
    "file_path = \"27624_word_7_cleaned_filtered_nam.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Define dependent variable and hierarchical predictors\n",
    "target_col = \"zRT_Nam_Zhang\"\n",
    "steps = {\n",
    "    \"Step1\": [\"Length\",\"NoS\", \"CF\",\"SUBTLEX_logW_CD\", \"NoWF\", \"NoM\", \"NoP\"],   # Basic lexical features\n",
    "    \"Step2\": [\"AoA\"],                                           # Age of Acquisition\n",
    "    \"Step3\": [\"Human_FAM_M_Su_log\"],                                 # Human familiarity\n",
    "    \"Step4\": [\"GPT_FAM_probs_log\"],                                  # GPT familiarity\n",
    "    \"Step5\": [\"GPT_FAM_probs_head_log\"]                                  # GPT head familiarity\n",
    "}\n",
    "\n",
    "# ====== 2. Data cleaning ======\n",
    "# Keep only the necessary columns\n",
    "cols_needed = [target_col] + [v for lst in steps.values() for v in lst]\n",
    "df = df[cols_needed].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna()\n",
    "print(f\"Valid sample size: N = {len(df)}\")\n",
    "\n",
    "# ====== 3. Standardization (for standardized β coefficients) ======\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "y = df_scaled[target_col]\n",
    "results = []\n",
    "prev_r2 = 0\n",
    "\n",
    "# ====== 4. Hierarchical regression loop ======\n",
    "for i, (step_name, vars_list) in enumerate(steps.items(), start=1):\n",
    "    # Include all variables up to the current step\n",
    "    all_vars = [v for step in list(steps.values())[:i] for v in step]\n",
    "    X = df_scaled[all_vars]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    r2 = model.rsquared\n",
    "    delta_r2 = r2 - prev_r2 if i > 1 else r2\n",
    "    prev_r2 = r2\n",
    "    \n",
    "    print(f\"\\n===== {step_name} =====\")\n",
    "    print(f\"R² = {r2:.3f} | ΔR² = {delta_r2:.3f}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Store summary results\n",
    "    for var in X.columns:\n",
    "        if var == \"const\":\n",
    "            continue\n",
    "        results.append({\n",
    "            \"Step\": step_name,\n",
    "            \"Variable\": var,\n",
    "            \"Beta\": round(model.params[var], 3),\n",
    "            \"SE\": round(model.bse[var], 3),\n",
    "            \"t\": round(model.tvalues[var], 3),\n",
    "            \"p\": round(model.pvalues[var], 4),\n",
    "            \"R2\": round(r2, 3),\n",
    "            \"ΔR2\": round(delta_r2, 3)\n",
    "        })\n",
    "\n",
    "# ====== 5. Combine and export results ======\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\n📊 Hierarchical Regression Summary:\")\n",
    "print(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f11f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid sample size: N = 978\n",
      "\n",
      "===== Step1 =====\n",
      "R² = 0.315 | ΔR² = 0.315\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        zRT_Nam_Hendrix   R-squared:                       0.315\n",
      "Model:                            OLS   Adj. R-squared:                  0.310\n",
      "Method:                 Least Squares   F-statistic:                     74.26\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):           2.78e-76\n",
      "Time:                        22:22:07   Log-Likelihood:                -1203.0\n",
      "No. Observations:                 978   AIC:                             2420.\n",
      "Df Residuals:                     971   BIC:                             2454.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "const            2.082e-17      0.027   7.83e-16      1.000      -0.052       0.052\n",
      "Length           1.313e-16    3.3e-17      3.977      0.000    6.65e-17    1.96e-16\n",
      "NoS                 0.1062      0.031      3.436      0.001       0.046       0.167\n",
      "CF                 -0.1254      0.047     -2.685      0.007      -0.217      -0.034\n",
      "SUBTLEX_logW_CD    -0.3010      0.028    -10.722      0.000      -0.356      -0.246\n",
      "NoWF               -0.2507      0.051     -4.952      0.000      -0.350      -0.151\n",
      "NoM                 0.0169      0.038      0.449      0.654      -0.057       0.091\n",
      "NoP                 0.0249      0.029      0.859      0.390      -0.032       0.082\n",
      "==============================================================================\n",
      "Omnibus:                      275.794   Durbin-Watson:                   1.479\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1012.395\n",
      "Skew:                           1.316   Prob(JB):                    1.45e-220\n",
      "Kurtosis:                       7.232   Cond. No.                     9.29e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 3.24e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "===== Step2 =====\n",
      "R² = 0.329 | ΔR² = 0.014\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        zRT_Nam_Hendrix   R-squared:                       0.329\n",
      "Model:                            OLS   Adj. R-squared:                  0.324\n",
      "Method:                 Least Squares   F-statistic:                     67.85\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):           1.12e-79\n",
      "Time:                        22:22:07   Log-Likelihood:                -1192.8\n",
      "No. Observations:                 978   AIC:                             2402.\n",
      "Df Residuals:                     970   BIC:                             2441.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "const            2.082e-17      0.026   7.91e-16      1.000      -0.052       0.052\n",
      "Length            1.12e-16   2.69e-17      4.164      0.000    5.92e-17    1.65e-16\n",
      "NoS                 0.1033      0.031      3.374      0.001       0.043       0.163\n",
      "CF                 -0.1390      0.046     -2.999      0.003      -0.230      -0.048\n",
      "SUBTLEX_logW_CD    -0.2538      0.030     -8.546      0.000      -0.312      -0.196\n",
      "NoWF               -0.2312      0.050     -4.595      0.000      -0.330      -0.132\n",
      "NoM                 0.0109      0.037      0.293      0.769      -0.062       0.084\n",
      "NoP                 0.0261      0.029      0.910      0.363      -0.030       0.082\n",
      "AoA                 0.1282      0.028      4.522      0.000       0.073       0.184\n",
      "==============================================================================\n",
      "Omnibus:                      276.505   Durbin-Watson:                   1.467\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1039.554\n",
      "Skew:                           1.311   Prob(JB):                    1.83e-226\n",
      "Kurtosis:                       7.317   Cond. No.                     1.82e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 8.53e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "===== Step3 =====\n",
      "R² = 0.357 | ΔR² = 0.028\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        zRT_Nam_Hendrix   R-squared:                       0.357\n",
      "Model:                            OLS   Adj. R-squared:                  0.352\n",
      "Method:                 Least Squares   F-statistic:                     67.26\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):           1.03e-87\n",
      "Time:                        22:22:07   Log-Likelihood:                -1171.7\n",
      "No. Observations:                 978   AIC:                             2361.\n",
      "Df Residuals:                     969   BIC:                             2405.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "const               2.082e-17      0.026   8.08e-16      1.000      -0.051       0.051\n",
      "Length              1.224e-17   9.45e-18      1.294      0.196   -6.32e-18    3.08e-17\n",
      "NoS                    0.1066      0.030      3.556      0.000       0.048       0.165\n",
      "CF                    -0.1453      0.045     -3.201      0.001      -0.234      -0.056\n",
      "SUBTLEX_logW_CD       -0.1651      0.032     -5.144      0.000      -0.228      -0.102\n",
      "NoWF                  -0.2220      0.049     -4.505      0.000      -0.319      -0.125\n",
      "NoM                    0.0042      0.037      0.114      0.910      -0.068       0.076\n",
      "NoP                    0.0219      0.028      0.778      0.437      -0.033       0.077\n",
      "AoA                    0.0691      0.029      2.367      0.018       0.012       0.126\n",
      "Human_FAM_M_Su_log    -0.2080      0.032     -6.536      0.000      -0.270      -0.146\n",
      "==============================================================================\n",
      "Omnibus:                      229.034   Durbin-Watson:                   1.463\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              763.402\n",
      "Skew:                           1.117   Prob(JB):                    1.70e-166\n",
      "Kurtosis:                       6.707   Cond. No.                     8.34e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 4.17e-31. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "===== Step4 =====\n",
      "R² = 0.391 | ΔR² = 0.034\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        zRT_Nam_Hendrix   R-squared:                       0.391\n",
      "Model:                            OLS   Adj. R-squared:                  0.385\n",
      "Method:                 Least Squares   F-statistic:                     69.09\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):           4.31e-98\n",
      "Time:                        22:22:07   Log-Likelihood:                -1145.1\n",
      "No. Observations:                 978   AIC:                             2310.\n",
      "Df Residuals:                     968   BIC:                             2359.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "const               2.082e-17      0.025    8.3e-16      1.000      -0.049       0.049\n",
      "Length              3.168e-17   2.12e-17      1.493      0.136   -9.95e-18    7.33e-17\n",
      "NoS                    0.1090      0.029      3.734      0.000       0.052       0.166\n",
      "CF                    -0.1677      0.044     -3.785      0.000      -0.255      -0.081\n",
      "SUBTLEX_logW_CD       -0.0380      0.036     -1.064      0.288      -0.108       0.032\n",
      "NoWF                  -0.1526      0.049     -3.121      0.002      -0.249      -0.057\n",
      "NoM                    0.0018      0.036      0.049      0.961      -0.068       0.072\n",
      "NoP                    0.0253      0.027      0.923      0.356      -0.028       0.079\n",
      "AoA                    0.0297      0.029      1.028      0.304      -0.027       0.086\n",
      "Human_FAM_M_Su_log    -0.1127      0.034     -3.355      0.001      -0.179      -0.047\n",
      "GPT_FAM_probs_log     -0.2933      0.040     -7.359      0.000      -0.372      -0.215\n",
      "==============================================================================\n",
      "Omnibus:                      208.490   Durbin-Watson:                   1.455\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              684.941\n",
      "Skew:                           1.021   Prob(JB):                    1.85e-149\n",
      "Kurtosis:                       6.555   Cond. No.                     2.21e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 6.68e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "===== Step5 =====\n",
      "R² = 0.442 | ΔR² = 0.051\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        zRT_Nam_Hendrix   R-squared:                       0.442\n",
      "Model:                            OLS   Adj. R-squared:                  0.437\n",
      "Method:                 Least Squares   F-statistic:                     76.75\n",
      "Date:                Sun, 26 Oct 2025   Prob (F-statistic):          1.86e-115\n",
      "Time:                        22:22:07   Log-Likelihood:                -1102.0\n",
      "No. Observations:                 978   AIC:                             2226.\n",
      "Df Residuals:                     967   BIC:                             2280.\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                   2.082e-17      0.024   8.67e-16      1.000      -0.047       0.047\n",
      "Length                 -5.895e-17   2.84e-17     -2.075      0.038   -1.15e-16    -3.2e-18\n",
      "NoS                        0.0964      0.028      3.448      0.001       0.042       0.151\n",
      "CF                        -0.1043      0.043     -2.429      0.015      -0.189      -0.020\n",
      "SUBTLEX_logW_CD           -0.0646      0.034     -1.882      0.060      -0.132       0.003\n",
      "NoWF                      -0.1121      0.047     -2.385      0.017      -0.204      -0.020\n",
      "NoM                        0.0350      0.034      1.024      0.306      -0.032       0.102\n",
      "NoP                        0.0199      0.026      0.760      0.447      -0.032       0.071\n",
      "AoA                        0.0435      0.028      1.568      0.117      -0.011       0.098\n",
      "Human_FAM_M_Su_log        -0.1259      0.032     -3.913      0.000      -0.189      -0.063\n",
      "GPT_FAM_probs_log         -0.1648      0.041     -4.067      0.000      -0.244      -0.085\n",
      "GPT_FAM_probs_head_log    -0.2922      0.031     -9.440      0.000      -0.353      -0.231\n",
      "==============================================================================\n",
      "Omnibus:                      182.257   Durbin-Watson:                   1.396\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              531.368\n",
      "Skew:                           0.932   Prob(JB):                    4.12e-116\n",
      "Kurtosis:                       6.093   Cond. No.                     1.73e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.22e-31. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "📊 Hierarchical Regression Summary:\n",
      "     Step                Variable   Beta     SE       t       p     R2    ΔR2\n",
      "0   Step1                  Length  0.000  0.000   3.977  0.0001  0.315  0.315\n",
      "1   Step1                     NoS  0.106  0.031   3.436  0.0006  0.315  0.315\n",
      "2   Step1                      CF -0.125  0.047  -2.685  0.0074  0.315  0.315\n",
      "3   Step1         SUBTLEX_logW_CD -0.301  0.028 -10.722  0.0000  0.315  0.315\n",
      "4   Step1                    NoWF -0.251  0.051  -4.952  0.0000  0.315  0.315\n",
      "5   Step1                     NoM  0.017  0.038   0.449  0.6535  0.315  0.315\n",
      "6   Step1                     NoP  0.025  0.029   0.859  0.3905  0.315  0.315\n",
      "7   Step2                  Length  0.000  0.000   4.164  0.0000  0.329  0.014\n",
      "8   Step2                     NoS  0.103  0.031   3.374  0.0008  0.329  0.014\n",
      "9   Step2                      CF -0.139  0.046  -2.999  0.0028  0.329  0.014\n",
      "10  Step2         SUBTLEX_logW_CD -0.254  0.030  -8.546  0.0000  0.329  0.014\n",
      "11  Step2                    NoWF -0.231  0.050  -4.595  0.0000  0.329  0.014\n",
      "12  Step2                     NoM  0.011  0.037   0.293  0.7694  0.329  0.014\n",
      "13  Step2                     NoP  0.026  0.029   0.910  0.3630  0.329  0.014\n",
      "14  Step2                     AoA  0.128  0.028   4.522  0.0000  0.329  0.014\n",
      "15  Step3                  Length  0.000  0.000   1.294  0.1960  0.357  0.028\n",
      "16  Step3                     NoS  0.107  0.030   3.556  0.0004  0.357  0.028\n",
      "17  Step3                      CF -0.145  0.045  -3.201  0.0014  0.357  0.028\n",
      "18  Step3         SUBTLEX_logW_CD -0.165  0.032  -5.144  0.0000  0.357  0.028\n",
      "19  Step3                    NoWF -0.222  0.049  -4.505  0.0000  0.357  0.028\n",
      "20  Step3                     NoM  0.004  0.037   0.114  0.9095  0.357  0.028\n",
      "21  Step3                     NoP  0.022  0.028   0.778  0.4368  0.357  0.028\n",
      "22  Step3                     AoA  0.069  0.029   2.367  0.0181  0.357  0.028\n",
      "23  Step3      Human_FAM_M_Su_log -0.208  0.032  -6.536  0.0000  0.357  0.028\n",
      "24  Step4                  Length  0.000  0.000   1.493  0.1357  0.391  0.034\n",
      "25  Step4                     NoS  0.109  0.029   3.734  0.0002  0.391  0.034\n",
      "26  Step4                      CF -0.168  0.044  -3.785  0.0002  0.391  0.034\n",
      "27  Step4         SUBTLEX_logW_CD -0.038  0.036  -1.064  0.2877  0.391  0.034\n",
      "28  Step4                    NoWF -0.153  0.049  -3.121  0.0019  0.391  0.034\n",
      "29  Step4                     NoM  0.002  0.036   0.049  0.9607  0.391  0.034\n",
      "30  Step4                     NoP  0.025  0.027   0.923  0.3564  0.391  0.034\n",
      "31  Step4                     AoA  0.030  0.029   1.028  0.3041  0.391  0.034\n",
      "32  Step4      Human_FAM_M_Su_log -0.113  0.034  -3.355  0.0008  0.391  0.034\n",
      "33  Step4       GPT_FAM_probs_log -0.293  0.040  -7.359  0.0000  0.391  0.034\n",
      "34  Step5                  Length -0.000  0.000  -2.075  0.0383  0.442  0.051\n",
      "35  Step5                     NoS  0.096  0.028   3.448  0.0006  0.442  0.051\n",
      "36  Step5                      CF -0.104  0.043  -2.429  0.0153  0.442  0.051\n",
      "37  Step5         SUBTLEX_logW_CD -0.065  0.034  -1.882  0.0601  0.442  0.051\n",
      "38  Step5                    NoWF -0.112  0.047  -2.385  0.0173  0.442  0.051\n",
      "39  Step5                     NoM  0.035  0.034   1.024  0.3062  0.442  0.051\n",
      "40  Step5                     NoP  0.020  0.026   0.760  0.4474  0.442  0.051\n",
      "41  Step5                     AoA  0.043  0.028   1.568  0.1173  0.442  0.051\n",
      "42  Step5      Human_FAM_M_Su_log -0.126  0.032  -3.913  0.0001  0.442  0.051\n",
      "43  Step5       GPT_FAM_probs_log -0.165  0.041  -4.067  0.0001  0.442  0.051\n",
      "44  Step5  GPT_FAM_probs_head_log -0.292  0.031  -9.440  0.0000  0.442  0.051\n"
     ]
    }
   ],
   "source": [
    "# # Hierarchical regression for Naming (multi-character)(Hendrix et al.,2020)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ====== 1. Load dataset ======\n",
    "file_path = \"27624_word_7_cleaned.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Define dependent variable and hierarchical predictors\n",
    "target_col = \"zRT_Nam_Hendrix\"\n",
    "steps = {\n",
    "    \"Step1\": [\"Length\",\"NoS\", \"CF\",\"SUBTLEX_logW_CD\", \"NoWF\", \"NoM\", \"NoP\"],   # Basic lexical features\n",
    "    \"Step2\": [\"AoA\"],                                           # Age of Acquisition\n",
    "    \"Step3\": [\"Human_FAM_M_Su_log\"],                                 # Human familiarity\n",
    "    \"Step4\": [\"GPT_FAM_probs_log\"],                                  # GPT familiarity\n",
    "    \"Step5\": [\"GPT_FAM_probs_head_log\"]                                  # GPT head familiarity\n",
    "}\n",
    "\n",
    "# ====== 2. Data cleaning ======\n",
    "# Keep only the necessary columns\n",
    "cols_needed = [target_col] + [v for lst in steps.values() for v in lst]\n",
    "df = df[cols_needed].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna()\n",
    "print(f\"Valid sample size: N = {len(df)}\")\n",
    "\n",
    "# ====== 3. Standardization (for standardized β coefficients) ======\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "y = df_scaled[target_col]\n",
    "results = []\n",
    "prev_r2 = 0\n",
    "\n",
    "# ====== 4. Hierarchical regression loop ======\n",
    "for i, (step_name, vars_list) in enumerate(steps.items(), start=1):\n",
    "    # Include all variables up to the current step\n",
    "    all_vars = [v for step in list(steps.values())[:i] for v in step]\n",
    "    X = df_scaled[all_vars]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    r2 = model.rsquared\n",
    "    delta_r2 = r2 - prev_r2 if i > 1 else r2\n",
    "    prev_r2 = r2\n",
    "    \n",
    "    print(f\"\\n===== {step_name} =====\")\n",
    "    print(f\"R² = {r2:.3f} | ΔR² = {delta_r2:.3f}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Store summary results\n",
    "    for var in X.columns:\n",
    "        if var == \"const\":\n",
    "            continue\n",
    "        results.append({\n",
    "            \"Step\": step_name,\n",
    "            \"Variable\": var,\n",
    "            \"Beta\": round(model.params[var], 3),\n",
    "            \"SE\": round(model.bse[var], 3),\n",
    "            \"t\": round(model.tvalues[var], 3),\n",
    "            \"p\": round(model.pvalues[var], 4),\n",
    "            \"R2\": round(r2, 3),\n",
    "            \"ΔR2\": round(delta_r2, 3)\n",
    "        })\n",
    "\n",
    "# ====== 5. Combine and export results ======\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\n📊 Hierarchical Regression Summary:\")\n",
    "print(res_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
