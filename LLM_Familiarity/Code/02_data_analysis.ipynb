{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b76cfa6-ef5e-486a-b7c1-f21c2e6e1a72",
   "metadata": {},
   "source": [
    "## **DATA AVAILABILITY NOTICE**\n",
    "##### This script requires external datasets that are not included in this repository due to copyright or privacy restrictions.\n",
    "##### Please refer to the citations in the manuscript to download the data or apply for access permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa8a99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149ae3f-aa87-4012-b8c1-a8c446f4b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_grouped_stats(df: pd.DataFrame, group_col: str, target_col: str):\n",
    "    stats = df.groupby(group_col)[target_col].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.float_format', '{:.4f}'.format) \n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(stats)\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "df = pd.read_excel('norms_gpt4o_character_prompt.xlsx') #norms_gpt4o_word_prompt.xlsx, norms_gpt4o_expression_prompt.xlsx\n",
    "print_grouped_stats(df, group_col='Length', target_col='GPT_FAM_probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63acbc35-a243-442c-b975-c8ee85bc961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_grouped_stats(df: pd.DataFrame, group_col: str, target_col: str):\n",
    "    stats = df.groupby(group_col)[target_col].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.float_format', '{:.4f}'.format) \n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(stats)\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "df = pd.read_excel('norms_qwen_max.xlsx')\n",
    "print_grouped_stats(df, group_col='Length', target_col='qwen_FAM_mean_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5af95-2a52-4e02-8431-a861f93dd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paired samples t-test\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def perform_paired_ttest(word_file: str, expr_file: str):\n",
    "\n",
    "    df_word = pd.read_excel(word_file)\n",
    "    df_expr = pd.read_excel(expr_file)\n",
    "\n",
    "    df_word_clean = df_word[['WORD', 'Length', 'GPT_FAM_probs']]\n",
    "    df_expr_clean = df_expr[['WORD', 'GPT_FAM_probs']].rename(\n",
    "        columns={'GPT_FAM_probs': 'GPT_FAM_probs_expr'}\n",
    "    )\n",
    "\n",
    "    merged_df = pd.merge(df_word_clean, df_expr_clean, on='WORD')\n",
    "\n",
    "    print(\"\\n>>> Paired Samples t-test Results by Length Group\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "    for length, group in merged_df.groupby('Length'):\n",
    "        \n",
    "        n = len(group)\n",
    "        df = n - 1\n",
    "        \n",
    "        t_stat, p_val = stats.ttest_rel(group['GPT_FAM_probs'], group['GPT_FAM_probs_expr'])\n",
    "        \n",
    "        diff = group['GPT_FAM_probs'] - group['GPT_FAM_probs_expr']\n",
    "        cohen_d = diff.mean() / diff.std(ddof=1)\n",
    "\n",
    "        mean_word, std_word = group['GPT_FAM_probs'].mean(), group['GPT_FAM_probs'].std()\n",
    "        mean_expr, std_expr = group['GPT_FAM_probs_expr'].mean(), group['GPT_FAM_probs_expr'].std()\n",
    "\n",
    "        print(f\"Group Length: {length} (N={n})\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Stats:  t = {t_stat:.4f} | p = {p_val:.4f} | df = {df}\")\n",
    "        print(f\"Effect: Cohen's d = {cohen_d:.4f}\")\n",
    "        print(f\"Word:   Mean = {mean_word:.4f}, Std = {std_word:.4f}\")\n",
    "        print(f\"Expr:   Mean = {mean_expr:.4f}, Std = {std_expr:.4f}\")\n",
    "        print(\"=\" * 65)\n",
    "\n",
    "WORD_FILE = 'norms_gpt4o_word_prompt.xlsx'\n",
    "EXPR_FILE = 'norms_gpt4o_expression_prompt.xlsx'\n",
    "\n",
    "perform_paired_ttest(WORD_FILE, EXPR_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb94e0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2dccd5-dd03-4348-aa02-7cf6300ce34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots of GPT-4o versus human familiarity ratings across word lengths\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "# File Paths \n",
    "PATH_CHAR = r\"D:\\0 ECNU\\CAILAB\\LLM_Familiarity\\Code\\norms_gpt4o_expression_prompt.xlsx\" \n",
    "PATH_WORD = r\"D:\\0 ECNU\\CAILAB\\LLM_familiarity\\Code\\norms_gpt4o_word_prompt.xlsx\" \n",
    "PATH_EXPR = r\"D:\\0 ECNU\\CAILAB\\LLM_familiarity\\Code\\norms_gpt4o_expression_prompt.xlsx\"\n",
    "\n",
    "# Visual Settings\n",
    "FONT_FAMILY = 'Arial' \n",
    "plt.rcParams['font.family'] = FONT_FAMILY\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Plotting Parameters\n",
    "AXIS_LIMITS = [0.8, 7.2]   # Extended range to prevent clipping\n",
    "TICKS = range(1, 8)        # Ticks: 1 to 7\n",
    "DOT_SIZE = 10              # Scatter dot size\n",
    "ALPHA = 0.7                # Transparency\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA LOADING & PREPROCESSING\n",
    "# ==========================================\n",
    "def load_and_filter(path, length_val=None, x_col='', y_col='', c_col='SUBTLEX_logWF'):\n",
    "    \"\"\"\n",
    "    Loads dataset, filters by length, and selects relevant columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(path)\n",
    "\n",
    "    # Filter by Length if specified\n",
    "    if length_val:\n",
    "        df = df[df['Length'] == length_val]\n",
    "    \n",
    "    # Ensure numeric types\n",
    "    cols = [x_col, y_col, c_col]\n",
    "    for c in cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    \n",
    "    return df[cols].dropna()\n",
    "\n",
    "# Load Datasets\n",
    "print(\"Loading datasets...\")\n",
    "df_A = load_and_filter(PATH_CHAR, 1, 'Human_FAM_Liu', 'GPT_FAM_probs')\n",
    "df_B = load_and_filter(PATH_WORD, 2, 'Human_FAM_Su', 'GPT_FAM_probs')\n",
    "df_C = load_and_filter(PATH_WORD, 3, 'Human_FAM_Su', 'GPT_FAM_probs')\n",
    "df_D = load_and_filter(PATH_EXPR, 4, 'Human_FAM_Su', 'GPT_FAM_probs')\n",
    "\n",
    "# global normalization for color mapping (Word Frequency)\n",
    "all_wf = pd.concat([df_A['SUBTLEX_logWF'], df_B['SUBTLEX_logWF'], \n",
    "                    df_C['SUBTLEX_logWF'], df_D['SUBTLEX_logWF']])\n",
    "norm = mcolors.Normalize(vmin=all_wf.min(), vmax=all_wf.max()) \n",
    "\n",
    "# ==========================================\n",
    "# 3. PLOTTING LOGIC\n",
    "# ==========================================\n",
    "# Initialize canvas\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4.5), sharey=True, constrained_layout=True)\n",
    "\n",
    "def plot_subplot(ax, df, x_col, title_label):\n",
    "    if df.empty:\n",
    "        ax.set_title(\"No Data\")\n",
    "        return None\n",
    "        \n",
    "    x = df[x_col]\n",
    "    y = df['GPT_FAM_probs']\n",
    "    c = df['SUBTLEX_logWF']\n",
    "    n = len(df)\n",
    "    \n",
    "    # 1. Scatter Plot\n",
    "    sc = ax.scatter(x, y, c=c, cmap='viridis', norm=norm, \n",
    "                    s=DOT_SIZE, alpha=ALPHA, edgecolors='none')\n",
    "    \n",
    "    # 2. Regression Line\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_range = np.linspace(x.min(), x.max(), 100) \n",
    "    ax.plot(x_range, p(x_range), 'k--', lw=1.5)\n",
    "    \n",
    "    # 3. Pearson Correlation\n",
    "    r, p_val = pearsonr(x, y)\n",
    "    star = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "    ax.text(0.95, 0.05, f\"$r$ = {r:.2f}{star}\", transform=ax.transAxes, \n",
    "            ha='right', va='bottom', fontsize=12, color='#1a3b32')\n",
    "    \n",
    "    # 4. Axis Formatting\n",
    "    ax.set_title(f\"{title_label} (N={n:,})\", fontsize=14, pad=10)\n",
    "    ax.set_xlim(AXIS_LIMITS)\n",
    "    ax.set_ylim(AXIS_LIMITS)\n",
    "    ax.set_xticks(TICKS)\n",
    "    ax.set_yticks(TICKS)\n",
    "    ax.set_box_aspect(1) \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Force display of Y-axis ticks/labels for all subplots (even with sharey=True)\n",
    "    ax.tick_params(axis='y', labelleft=True, left=True)\n",
    "    \n",
    "    return sc\n",
    "\n",
    "# Generate Subplots\n",
    "print(\"Generating plots...\")\n",
    "sc_A = plot_subplot(axes[0], df_A, 'Human_FAM_Liu', 'Word length = 1')\n",
    "sc_B = plot_subplot(axes[1], df_B, 'Human_FAM_Su', 'Word length = 2')\n",
    "sc_C = plot_subplot(axes[2], df_C, 'Human_FAM_Su', 'Word length = 3')\n",
    "sc_D = plot_subplot(axes[3], df_D, 'Human_FAM_Su', 'Word length = 4')\n",
    "\n",
    "# ==========================================\n",
    "# 4. FINAL TOUCHES & DISPLAY\n",
    "# ==========================================\n",
    "# Global Labels (Adjusted positions as requested)\n",
    "fig.supxlabel('Human FAM', fontsize=16, y=-0.06)\n",
    "fig.supylabel('GPT FAM', fontsize=16, x=-0.02) \n",
    "\n",
    "# Colorbar (Using the last plot's mappable)\n",
    "if sc_D:\n",
    "    cbar = fig.colorbar(sc_D, ax=axes, orientation='vertical', fraction=0.02, pad=0.02)\n",
    "    cbar.set_label('Word Frequency (Log WF)', fontsize=12, rotation=270, labelpad=20)\n",
    "\n",
    "print(\"Displaying plot window...\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bivariate correlations by word length - single-character items\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_excel('norms_gpt4o_expression_prompt.xlsx')  \n",
    "\n",
    "# Group by Length and compute Pearson correlation per group\n",
    "results = []\n",
    "for name, group in data.groupby('Length'):\n",
    "    # Drop rows with NaN or infinite values in the relevant columns\n",
    "    group = group.replace([np.inf, -np.inf], np.nan).dropna(subset=['GPT_FAM_probs','Human_FAM_Liu']) # 'SUBTLEX_logWF','SUBTLEX_logW_CD','SUBTLEX_logCHR','SUBTLEX_logCHR_CD'(å½“ç®—å’Œè¯é¢‘çš„ç›¸å…³çš„æ—¶å€™)\n",
    "    \n",
    "    # Compute sample size\n",
    "    sample_size = group.shape[0]\n",
    "\n",
    "    # Require at least 2 observations to compute correlation\n",
    "    if sample_size > 1:\n",
    "        # Compute Pearson r and p-value, format to 4 decimals\n",
    "        r, p_value = pearsonr(group['GPT_FAM_probs'], group['Human_FAM_Liu'])\n",
    "        results.append((name, sample_size, f\"{r:.4f}\", f\"{p_value:.4f}\"))\n",
    "    else:\n",
    "        # Not enough data for this length\n",
    "        results.append((name, sample_size, \"N/A\", \"N/A\"))\n",
    "\n",
    "# Convert results to DataFrame for display\n",
    "result_df = pd.DataFrame(results, columns=['WordLength', 'SampleSize', 'Pearsonr', 'PValue'])\n",
    "\n",
    "# Print results\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bivariate correlations by word length - multi-character items\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel('norms_gpt4o_word_prompt.xlsx')  \n",
    "\n",
    "# Group by Length and compute correlation per group\n",
    "results = []\n",
    "for name, group in data.groupby('Length'):\n",
    "    # Remove rows with NaN or infinite values (keep relevant English tokens)\n",
    "    group = group.replace([np.inf, -np.inf], np.nan).dropna(subset=['GPT_FAM_probs', 'Human_FAM_M_Su']) # 'SUBTLEX_logWF','SUBTLEX_logW_CD'\n",
    "    \n",
    "    # Compute sample size\n",
    "    sample_size = group.shape[0]\n",
    "\n",
    "    # Require at least 2 observations to compute Pearson r\n",
    "    if sample_size > 1:\n",
    "        r, p_value = pearsonr(group['GPT_FAM_probs'], group['Human_FAM_M_Su'])\n",
    "        # Format r and p-value to 3 decimals\n",
    "        results.append((name, sample_size, f\"{r:.3f}\", f\"{p_value:.3f}\"))\n",
    "    else:\n",
    "        # Not enough data for this length\n",
    "        results.append((name, sample_size, \"N/A\", \"N/A\"))\n",
    "\n",
    "# Convert results to DataFrame for display\n",
    "result_df = pd.DataFrame(results, columns=['WordLength', 'SampleSize', 'Pearsonr', 'PValue'])\n",
    "\n",
    "# Print results\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2510c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute partial correlation controlling for Length\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "\n",
    "# Load Excel file into a DataFrame\n",
    "file_path = 'norms_gpt4o_word_prompt.xlsx' \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Drop rows missing the variables needed for the analysis\n",
    "df_cleaned = df.dropna(subset=['GPT_FAM_probs', 'Human_FAM_M_Su', 'Length'])\n",
    "\n",
    "# Compute partial correlation controlling for Length\n",
    "partial_corr_result = pg.partial_corr(data=df_cleaned, x='GPT_FAM_probs', y='Human_FAM_M_Su', covar='Length')\n",
    "\n",
    "# Print the partial correlation result; format p-value to 4 decimal places\n",
    "print(partial_corr_result.to_string(formatters={'p-val': '{:.4f}'.format}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b3d38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c693d",
   "metadata": {},
   "source": [
    "## Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate linear regression - single-character items - LDT task\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data file and specify columns to read\n",
    "file_path = '27624_expression_7_cleaned_filtered.xlsx'  # Excel file path\n",
    "y_column = 'zRT_LDT'  # dependent variable\n",
    "x_columns = [\"GPT_FAM_probs\",\"qwen_FAM_mean_30\",\"Human_FAM_Liu\",\"SUBTLEX_logWF\",\"SUBTLEX_logW_CD\",\"SUBTLEX_logCHR\",\"SUBTLEX_logCHR_CD\"]  # predictors\n",
    "length_column = 'Length'  # word length column\n",
    "\n",
    "# Read Excel and select needed columns\n",
    "data = pd.read_excel(file_path, usecols=x_columns + [y_column, length_column])\n",
    "\n",
    "# Remove rows with missing values in any predictor or the outcome\n",
    "data_cleaned = data.dropna(subset=x_columns + [y_column])\n",
    "\n",
    "# Collect regression results\n",
    "final_results = []\n",
    "\n",
    "# Loop over each Length group and run univariate OLS for each predictor\n",
    "lengths = data_cleaned[length_column].unique()\n",
    "for length in lengths:\n",
    "    length_data = data_cleaned[data_cleaned[length_column] == length]\n",
    "    \n",
    "    for x in x_columns:\n",
    "        formula = f'{y_column} ~ {x}'\n",
    "        model = smf.ols(formula, data=length_data).fit()\n",
    "        \n",
    "        final_results.append({\n",
    "            'Length': length,\n",
    "            'Variable': x,\n",
    "            'R-squared': round(model.rsquared, 3),\n",
    "            'Coefficient': round(model.params[x], 3),\n",
    "            'P-value': round(model.pvalues[x], 4),\n",
    "            'Standard Error': round(model.bse[x], 3),\n",
    "            'Sample Size': int(model.nobs)\n",
    "        })\n",
    "\n",
    "# Convert results list to DataFrame and display\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate linear regression - single-character items - Naming task\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data\n",
    "file_path = '27624_expression_7_cleaned.xlsx'  \n",
    "y_column = 'zRT_Nam_Liu'  # Dependent variable column name\n",
    "x_columns = [\"GPT_FAM_probs\",\"qwen_FAM_mean_30\",\"Human_FAM_Liu\",\"SUBTLEX_logWF\",\"SUBTLEX_logW_CD\",\"SUBTLEX_logCHR\",\"SUBTLEX_logCHR_CD\"]  # Predictor variable names\n",
    "length_column = 'Length'  # Word length column nam\n",
    "\n",
    "# Read Excel file (select columns)\n",
    "data = pd.read_excel(file_path, usecols=x_columns + [y_column, length_column])\n",
    "\n",
    "# Drop rows with missing values in predictors or outcome\n",
    "data_cleaned = data.dropna(subset=x_columns + [y_column])\n",
    "\n",
    "# Store regression results for each length\n",
    "final_results = []\n",
    "\n",
    "# Iterate over word lengths\n",
    "lengths = data_cleaned[length_column].unique()  # Get unique lengths\n",
    "for length in lengths:\n",
    "    # Filter data for current length\n",
    "    length_data = data_cleaned[data_cleaned[length_column] == length]\n",
    "    \n",
    "    # Run univariate regression for each predictor\n",
    "    for x in x_columns:\n",
    "        # Fit OLS model\n",
    "        formula = f'{y_column} ~ {x}'\n",
    "        model = smf.ols(formula, data=length_data).fit()\n",
    "        \n",
    "        # Save metrics\n",
    "        final_results.append({\n",
    "            'Length': length,\n",
    "            'Variable': x,\n",
    "            'R-squared': round(model.rsquared, 3),\n",
    "            'Coefficient': round(model.params[x], 3),\n",
    "            'P-value': round(model.pvalues[x], 4),\n",
    "            'Standard Error': round(model.bse[x], 3),\n",
    "            'Sample Size': int(model.nobs)  # Add sample size\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Print results\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d94344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate linear regression - multi-character words - LDT task\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data\n",
    "file_path = '27624_word_7_cleaned_filtered.xlsx'  \n",
    "y_column = 'zRT_LDT' \n",
    "x_columns = [\"GPT_FAM_probs\",\"qwen_FAM_mean_30\",\"Human_FAM_M_Su\",\"SUBTLEX_logWF\",\"SUBTLEX_logW_CD\",\"GPT_FAM_probs_head\",\"SUBTLEX_logCHR_head\",\"SUBTLEX_logCHR_CD_head\"]  \n",
    "length_column = 'Length'  \n",
    "\n",
    "# Read Excel file\n",
    "data = pd.read_excel(file_path, usecols=x_columns + [y_column, length_column])\n",
    "\n",
    "# Clean dataset: keep rows with no missing values in predictors and outcome\n",
    "data_cleaned = data.dropna(subset=x_columns + [y_column])\n",
    "\n",
    "# Store regression results per word length\n",
    "final_results = []\n",
    "\n",
    "# Group by word length\n",
    "lengths = data_cleaned[length_column].unique()  \n",
    "for length in lengths:\n",
    "    # Filter data for the current length\n",
    "    length_data = data_cleaned[data_cleaned[length_column] == length]\n",
    "    \n",
    "    # Run univariate regression for each predictor\n",
    "    for x in x_columns:\n",
    "        # Fit OLS model for y ~ x\n",
    "        formula = f'{y_column} ~ {x}'\n",
    "        model = smf.ols(formula, data=length_data).fit()\n",
    "        \n",
    "        # Save regression metrics\n",
    "        final_results.append({\n",
    "            'Length': length,\n",
    "            'Variable': x,\n",
    "            'R-squared': round(model.rsquared, 3),\n",
    "            'Coefficient': round(model.params[x], 3),\n",
    "            'P-value': round(model.pvalues[x], 4),\n",
    "            'Standard Error': round(model.bse[x], 3),\n",
    "            'Sample Size': int(model.nobs)  # æ·»åŠ æ ·æœ¬é‡\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Print results\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51af50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate linear regression - multi-character words - Naming task -Zhang et al.,2023\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data\n",
    "file_path = '27624_word_7_cleaned_filtered_nam.xlsx'  \n",
    "y_column = 'zRT_Nam_Zhang'   \n",
    "x_columns = [\"GPT_FAM_probs\",\"qwen_FAM_mean_30\",\"Human_FAM_M_Su\",\"SUBTLEX_logWF\",\"SUBTLEX_logW_CD\",\"GPT_FAM_probs_head\",\"SUBTLEX_logCHR_head\",\"SUBTLEX_logCHR_CD_head\"]  \n",
    "length_column = 'Length'  \n",
    "\n",
    "# Read Excel file\n",
    "data = pd.read_excel(file_path, usecols=x_columns + [y_column, length_column])\n",
    "\n",
    "# Ensure rows include no missing values for all predictors and the outcome\n",
    "data_cleaned = data.dropna(subset=x_columns + [y_column])\n",
    "\n",
    "# Store regression results\n",
    "final_results = []\n",
    "\n",
    "# Iterate over word lengths and run univariate regression for each predictor\n",
    "lengths = data_cleaned[length_column].unique()  \n",
    "for length in lengths:\n",
    "    # Filter data for current length\n",
    "    length_data = data_cleaned[data_cleaned[length_column] == length]\n",
    "    \n",
    "    # Run univariate OLS for each predictor\n",
    "    for x in x_columns:\n",
    "        # Fit the model y ~ x\n",
    "        formula = f'{y_column} ~ {x}'\n",
    "        model = smf.ols(formula, data=length_data).fit()\n",
    "        \n",
    "        # Save regression metrics\n",
    "        final_results.append({\n",
    "            'Length': length,\n",
    "            'Variable': x,\n",
    "            'R-squared': round(model.rsquared, 3),\n",
    "            'Coefficient': round(model.params[x], 3),\n",
    "            'P-value': round(model.pvalues[x], 4),\n",
    "            'Standard Error': round(model.bse[x], 3),\n",
    "            'Sample Size': int(model.nobs)  \n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and print\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Print results\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate linear regression - multi-character words - Naming task\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load data\n",
    "file_path = '27624_word_7_cleaned.xlsx'  \n",
    "y_column = 'zRT_Nam_Hendrix'  # zRT_Nam_Hendrix \n",
    "x_columns = [\"GPT_FAM_probs\",\"qwen_FAM_mean_30\",\"Human_FAM_M_Su\",\"SUBTLEX_logWF\",\"SUBTLEX_logW_CD\",\"GPT_FAM_probs_head\",\"SUBTLEX_logCHR_head\",\"SUBTLEX_logCHR_CD_head\"]  \n",
    "length_column = 'Length'  \n",
    "\n",
    "# Read Excel file\n",
    "data = pd.read_excel(file_path, usecols=x_columns + [y_column, length_column])\n",
    "\n",
    "# Ensure rows include no missing values for all predictors and the outcome\n",
    "data_cleaned = data.dropna(subset=x_columns + [y_column])\n",
    "\n",
    "# Store regression results\n",
    "final_results = []\n",
    "\n",
    "# Iterate over word lengths and run univariate regression for each predictor\n",
    "lengths = data_cleaned[length_column].unique()  \n",
    "for length in lengths:\n",
    "    # Filter data for current length\n",
    "    length_data = data_cleaned[data_cleaned[length_column] == length]\n",
    "    \n",
    "    # Run univariate OLS for each predictor\n",
    "    for x in x_columns:\n",
    "        # Fit the model y ~ x\n",
    "        formula = f'{y_column} ~ {x}'\n",
    "        model = smf.ols(formula, data=length_data).fit()\n",
    "        \n",
    "        # Save regression metrics\n",
    "        final_results.append({\n",
    "            'Length': length,\n",
    "            'Variable': x,\n",
    "            'R-squared': round(model.rsquared, 3),\n",
    "            'Coefficient': round(model.params[x], 3),\n",
    "            'P-value': round(model.pvalues[x], 4),\n",
    "            'Standard Error': round(model.bse[x], 3),\n",
    "            'Sample Size': int(model.nobs)  \n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and print\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Print results\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e81568",
   "metadata": {},
   "source": [
    "## Univariate Nonlinear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Nonlinear regression - single-character words - LDT task\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix\n",
    "\n",
    "# --------- Step 1: Read data ---------\n",
    "FILE_PATH = r\"D:/0 ECNU/CAILAB/LLM_familiarity/Data/27624_expression_7_cleaned_filtered.xlsx\"  # modify if needed\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# --------- Step 2: Define variables ---------\n",
    "y_column = 'zRT_LDT'\n",
    "x_columns = [\n",
    "    'GPT_FAM_probs',\n",
    "    'qwen_FAM_mean_30',\n",
    "    'Human_FAM_Liu',\n",
    "    'SUBTLEX_logWF',\n",
    "    'SUBTLEX_logW_CD',\n",
    "    'SUBTLEX_logCHR',\n",
    "    'SUBTLEX_logCHR_CD'\n",
    "]\n",
    "\n",
    "# Keep rows with complete cases on selected vars (+ Length)\n",
    "needed_cols = [y_column, 'Length'] + x_columns\n",
    "df_clean = df.loc[:, needed_cols].copy()\n",
    "\n",
    "# --------- Step 2.1: Ensure predictors are numeric ---------\n",
    "# Convert all selected predictors to numeric (coerce non-numeric to NaN)\n",
    "for col in x_columns + [y_column]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Drop rows with any NaNs in required columns\n",
    "df_clean = df_clean.dropna(subset=needed_cols).reset_index(drop=True)\n",
    "\n",
    "# Safety check\n",
    "non_numeric = [c for c in x_columns if not np.issubdtype(df_clean[c].dtype, np.number)]\n",
    "if len(non_numeric) > 0:\n",
    "    raise ValueError(f\"Some variables could not be converted to numeric: {', '.join(non_numeric)}\")\n",
    "\n",
    "# --------- Step 3: Unique word lengths ---------\n",
    "lengths = sorted(df_clean['Length'].unique())\n",
    "\n",
    "# --------- Prepare final results ---------\n",
    "final_results = []\n",
    "\n",
    "# --------- Step 4~8: Loop by Length and predictor; fit OLS with spline ---------\n",
    "for length_value in lengths:\n",
    "    length_data = df_clean[df_clean['Length'] == length_value].copy()\n",
    "    # Skip tiny groups\n",
    "    if len(length_data) < 5:\n",
    "        print(f\"[Skip] Length={length_value}: sample too small (n={len(length_data)})\")\n",
    "        continue\n",
    "\n",
    "    for x in x_columns:\n",
    "        # Build formula: y ~ cr(x, df=4)\n",
    "        # patsy/statsmodels support cr() via patsy; use smf.ols with formula\n",
    "        formula = f\"{y_column} ~ cr({x}, df=4)\"\n",
    "\n",
    "        try:\n",
    "            fit = smf.ols(formula, data=length_data).fit()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Length={length_value}, Var={x}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --------- Step 8: Collect stats ---------\n",
    "        final_results.append({\n",
    "            \"Length\": length_value,\n",
    "            \"Variable\": x,\n",
    "            \"R_squared\": round(fit.rsquared, 3),\n",
    "            \"P_value\": round(float(fit.f_pvalue), 4),\n",
    "            \"Sample_Size\": int(len(length_data))\n",
    "        })\n",
    "\n",
    "# --------- Step 9: Print final results table ---------\n",
    "final_df = pd.DataFrame(final_results)\n",
    "if not final_df.empty:\n",
    "    final_df[\"Variable\"] = pd.Categorical(final_df[\"Variable\"], categories=x_columns, ordered=True)\n",
    "    final_df = final_df.sort_values(by=[\"Length\", \"Variable\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\næœ€ç»ˆçš„å›žå½’ç»“æžœè¡¨:\")\n",
    "    print(final_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo models were fitted (empty results).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883cb9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Nonlinear regression - single-character words - Naming task\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix\n",
    "\n",
    "# --------- Step 1: Read data ---------\n",
    "FILE_PATH = r\"D:/0 ECNU/CAILAB/LLM_familiarity/Data/27624_expression_7_cleaned.xlsx\"  # modify if needed\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# --------- Step 2: Define variables ---------\n",
    "y_column = 'zRT_Nam_Liu'\n",
    "x_columns = [\n",
    "    'GPT_FAM_probs',\n",
    "    'qwen_FAM_mean_30',\n",
    "    'Human_FAM_Liu',\n",
    "    'SUBTLEX_logWF',\n",
    "    'SUBTLEX_logW_CD',\n",
    "    'SUBTLEX_logCHR',\n",
    "    'SUBTLEX_logCHR_CD'\n",
    "]\n",
    "\n",
    "# Keep rows with complete cases on selected vars (+ Length)\n",
    "needed_cols = [y_column, 'Length'] + x_columns\n",
    "df_clean = df.loc[:, needed_cols].copy()\n",
    "\n",
    "# --------- Step 2.1: Ensure predictors are numeric ---------\n",
    "# Convert all selected predictors to numeric (coerce non-numeric to NaN)\n",
    "for col in x_columns + [y_column]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Drop rows with any NaNs in required columns\n",
    "df_clean = df_clean.dropna(subset=needed_cols).reset_index(drop=True)\n",
    "\n",
    "# Safety check\n",
    "non_numeric = [c for c in x_columns if not np.issubdtype(df_clean[c].dtype, np.number)]\n",
    "if len(non_numeric) > 0:\n",
    "    raise ValueError(f\"Some variables could not be converted to numeric: {', '.join(non_numeric)}\")\n",
    "\n",
    "# --------- Step 3: Unique word lengths ---------\n",
    "lengths = sorted(df_clean['Length'].unique())\n",
    "\n",
    "# --------- Prepare final results ---------\n",
    "final_results = []\n",
    "\n",
    "# --------- Step 4~8: Loop by Length and predictor; fit OLS with spline ---------\n",
    "for length_value in lengths:\n",
    "    length_data = df_clean[df_clean['Length'] == length_value].copy()\n",
    "    # Skip tiny groups\n",
    "    if len(length_data) < 5:\n",
    "        print(f\"[Skip] Length={length_value}: sample too small (n={len(length_data)})\")\n",
    "        continue\n",
    "\n",
    "    for x in x_columns:\n",
    "        # Build formula: y ~ cr(x, df=4)\n",
    "        # patsy/statsmodels support cr() via patsy; use smf.ols with formula\n",
    "        formula = f\"{y_column} ~ cr({x}, df=4)\"\n",
    "\n",
    "        try:\n",
    "            fit = smf.ols(formula, data=length_data).fit()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Length={length_value}, Var={x}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --------- Step 8: Collect stats ---------\n",
    "        final_results.append({\n",
    "            \"Length\": length_value,\n",
    "            \"Variable\": x,\n",
    "            \"R_squared\": round(fit.rsquared, 3),\n",
    "            \"P_value\": round(float(fit.f_pvalue), 4),\n",
    "            \"Sample_Size\": int(len(length_data))\n",
    "        })\n",
    "\n",
    "# --------- Step 9: Print final results table ---------\n",
    "final_df = pd.DataFrame(final_results)\n",
    "if not final_df.empty:\n",
    "    final_df[\"Variable\"] = pd.Categorical(final_df[\"Variable\"], categories=x_columns, ordered=True)\n",
    "    final_df = final_df.sort_values(by=[\"Length\", \"Variable\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\næœ€ç»ˆçš„å›žå½’ç»“æžœè¡¨:\")\n",
    "    print(final_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo models were fitted (empty results).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041219e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Nonlinear regression - multi-character words - LDT task\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix\n",
    "\n",
    "# --------- Step 1: Read data ---------\n",
    "FILE_PATH = r\"D:/0 ECNU/CAILAB/LLM_familiarity/Data/27624_word_7_cleaned_filtered.xlsx\"  # modify if needed\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# --------- Step 2: Define variables ---------\n",
    "y_column = 'zRT_LDT'\n",
    "x_columns = [\n",
    "    'GPT_FAM_probs',\n",
    "    'qwen_FAM_mean_30',\n",
    "    'Human_FAM_M_Su',\n",
    "    'SUBTLEX_logWF',\n",
    "    'SUBTLEX_logW_CD',\n",
    "    'GPT_FAM_probs_head',\n",
    "    'SUBTLEX_logCHR_head',\n",
    "    'SUBTLEX_logCHR_CD_head'\n",
    "]\n",
    "\n",
    "# Keep rows with complete cases on selected vars (+ Length)\n",
    "needed_cols = [y_column, 'Length'] + x_columns\n",
    "df_clean = df.loc[:, needed_cols].copy()\n",
    "\n",
    "# --------- Step 2.1: Ensure predictors are numeric ---------\n",
    "# Convert all selected predictors to numeric (coerce non-numeric to NaN)\n",
    "for col in x_columns + [y_column]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Drop rows with any NaNs in required columns\n",
    "df_clean = df_clean.dropna(subset=needed_cols).reset_index(drop=True)\n",
    "\n",
    "# Safety check\n",
    "non_numeric = [c for c in x_columns if not np.issubdtype(df_clean[c].dtype, np.number)]\n",
    "if len(non_numeric) > 0:\n",
    "    raise ValueError(f\"Some variables could not be converted to numeric: {', '.join(non_numeric)}\")\n",
    "\n",
    "# --------- Step 3: Unique word lengths ---------\n",
    "lengths = sorted(df_clean['Length'].unique())\n",
    "\n",
    "# --------- Prepare final results ---------\n",
    "final_results = []\n",
    "\n",
    "# --------- Step 4~8: Loop by Length and predictor; fit OLS with spline ---------\n",
    "for length_value in lengths:\n",
    "    length_data = df_clean[df_clean['Length'] == length_value].copy()\n",
    "    # Skip tiny groups\n",
    "    if len(length_data) < 5:\n",
    "        print(f\"[Skip] Length={length_value}: sample too small (n={len(length_data)})\")\n",
    "        continue\n",
    "\n",
    "    for x in x_columns:\n",
    "        # Build formula: y ~ cr(x, df=4)\n",
    "        # patsy/statsmodels support cr() via patsy; use smf.ols with formula\n",
    "        formula = f\"{y_column} ~ cr({x}, df=4)\"\n",
    "\n",
    "        try:\n",
    "            fit = smf.ols(formula, data=length_data).fit()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Length={length_value}, Var={x}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --------- Step 8: Collect stats ---------\n",
    "        final_results.append({\n",
    "            \"Length\": length_value,\n",
    "            \"Variable\": x,\n",
    "            \"R_squared\": round(fit.rsquared, 3),\n",
    "            \"P_value\": round(float(fit.f_pvalue), 4),\n",
    "            \"Sample_Size\": int(len(length_data))\n",
    "        })\n",
    "\n",
    "# --------- Step 9: Print final results table ---------\n",
    "final_df = pd.DataFrame(final_results)\n",
    "if not final_df.empty:\n",
    "    final_df[\"Variable\"] = pd.Categorical(final_df[\"Variable\"], categories=x_columns, ordered=True)\n",
    "    final_df = final_df.sort_values(by=[\"Length\", \"Variable\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\næœ€ç»ˆçš„å›žå½’ç»“æžœè¡¨:\")\n",
    "    print(final_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo models were fitted (empty results).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Nonlinear regression - multi-character words - Naming task - Zhang et al.,2023\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix\n",
    "\n",
    "# --------- Step 1: Read data ---------\n",
    "FILE_PATH = r\"D:/0 ECNU/CAILAB/LLM_familiarity/Data/27624_word_7_cleaned_filtered_nam.xlsx\"  # modify if needed\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# --------- Step 2: Define variables ---------\n",
    "y_column = 'zRT_Nam_Zhang'\n",
    "x_columns = [\n",
    "    'GPT_FAM_probs',\n",
    "    'qwen_FAM_mean_30',\n",
    "    'Human_FAM_M_Su',\n",
    "    'SUBTLEX_logWF',\n",
    "    'SUBTLEX_logW_CD',\n",
    "    'GPT_FAM_probs_head',\n",
    "    'SUBTLEX_logCHR_head',\n",
    "    'SUBTLEX_logCHR_CD_head'\n",
    "]\n",
    "\n",
    "# Keep rows with complete cases on selected vars (+ Length)\n",
    "needed_cols = [y_column, 'Length'] + x_columns\n",
    "df_clean = df.loc[:, needed_cols].copy()\n",
    "\n",
    "# --------- Step 2.1: Ensure predictors are numeric ---------\n",
    "# Convert all selected predictors to numeric (coerce non-numeric to NaN)\n",
    "for col in x_columns + [y_column]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Drop rows with any NaNs in required columns\n",
    "df_clean = df_clean.dropna(subset=needed_cols).reset_index(drop=True)\n",
    "\n",
    "# Safety check\n",
    "non_numeric = [c for c in x_columns if not np.issubdtype(df_clean[c].dtype, np.number)]\n",
    "if len(non_numeric) > 0:\n",
    "    raise ValueError(f\"Some variables could not be converted to numeric: {', '.join(non_numeric)}\")\n",
    "\n",
    "# --------- Step 3: Unique word lengths ---------\n",
    "lengths = sorted(df_clean['Length'].unique())\n",
    "\n",
    "# --------- Prepare final results ---------\n",
    "final_results = []\n",
    "\n",
    "# --------- Step 4~8: Loop by Length and predictor; fit OLS with spline ---------\n",
    "for length_value in lengths:\n",
    "    length_data = df_clean[df_clean['Length'] == length_value].copy()\n",
    "    # Skip tiny groups\n",
    "    if len(length_data) < 5:\n",
    "        print(f\"[Skip] Length={length_value}: sample too small (n={len(length_data)})\")\n",
    "        continue\n",
    "\n",
    "    for x in x_columns:\n",
    "        # Build formula: y ~ cr(x, df=4)\n",
    "        # patsy/statsmodels support cr() via patsy; use smf.ols with formula\n",
    "        formula = f\"{y_column} ~ cr({x}, df=4)\"\n",
    "\n",
    "        try:\n",
    "            fit = smf.ols(formula, data=length_data).fit()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Length={length_value}, Var={x}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --------- Step 8: Collect stats ---------\n",
    "        final_results.append({\n",
    "            \"Length\": length_value,\n",
    "            \"Variable\": x,\n",
    "            \"R_squared\": round(fit.rsquared, 3),\n",
    "            \"P_value\": round(float(fit.f_pvalue), 4),\n",
    "            \"Sample_Size\": int(len(length_data))\n",
    "        })\n",
    "\n",
    "# --------- Step 9: Print final results table ---------\n",
    "final_df = pd.DataFrame(final_results)\n",
    "if not final_df.empty:\n",
    "    final_df[\"Variable\"] = pd.Categorical(final_df[\"Variable\"], categories=x_columns, ordered=True)\n",
    "    final_df = final_df.sort_values(by=[\"Length\", \"Variable\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\næœ€ç»ˆçš„å›žå½’ç»“æžœè¡¨:\")\n",
    "    print(final_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo models were fitted (empty results).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Nonlinear regression - multi-character words - Naming task - Hendrix et al.,2022\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrix\n",
    "\n",
    "# --------- Step 1: Read data ---------\n",
    "FILE_PATH = r\"D:/0 ECNU/CAILAB/LLM_familiarity/Data/27624_word_7_cleaned.xlsx\"  # modify if needed\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# --------- Step 2: Define variables ---------\n",
    "y_column = 'zRT_Nam_Hendrix'\n",
    "x_columns = [\n",
    "    'GPT_FAM_probs',\n",
    "    'qwen_FAM_mean_30',\n",
    "    'Human_FAM_M_Su',\n",
    "    'SUBTLEX_logWF',\n",
    "    'SUBTLEX_logW_CD',\n",
    "    'GPT_FAM_probs_head',\n",
    "    'SUBTLEX_logCHR_head',\n",
    "    'SUBTLEX_logCHR_CD_head'\n",
    "]\n",
    "\n",
    "# Keep rows with complete cases on selected vars (+ Length)\n",
    "needed_cols = [y_column, 'Length'] + x_columns\n",
    "df_clean = df.loc[:, needed_cols].copy()\n",
    "\n",
    "# --------- Step 2.1: Ensure predictors are numeric ---------\n",
    "# Convert all selected predictors to numeric (coerce non-numeric to NaN)\n",
    "for col in x_columns + [y_column]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Drop rows with any NaNs in required columns\n",
    "df_clean = df_clean.dropna(subset=needed_cols).reset_index(drop=True)\n",
    "\n",
    "# Safety check\n",
    "non_numeric = [c for c in x_columns if not np.issubdtype(df_clean[c].dtype, np.number)]\n",
    "if len(non_numeric) > 0:\n",
    "    raise ValueError(f\"Some variables could not be converted to numeric: {', '.join(non_numeric)}\")\n",
    "\n",
    "# --------- Step 3: Unique word lengths ---------\n",
    "lengths = sorted(df_clean['Length'].unique())\n",
    "\n",
    "# --------- Prepare final results ---------\n",
    "final_results = []\n",
    "\n",
    "# --------- Step 4~8: Loop by Length and predictor; fit OLS with spline ---------\n",
    "for length_value in lengths:\n",
    "    length_data = df_clean[df_clean['Length'] == length_value].copy()\n",
    "    # Skip tiny groups\n",
    "    if len(length_data) < 5:\n",
    "        print(f\"[Skip] Length={length_value}: sample too small (n={len(length_data)})\")\n",
    "        continue\n",
    "\n",
    "    for x in x_columns:\n",
    "        # Build formula: y ~ cr(x, df=4)\n",
    "        # patsy/statsmodels support cr() via patsy; use smf.ols with formula\n",
    "        formula = f\"{y_column} ~ cr({x}, df=4)\"\n",
    "\n",
    "        try:\n",
    "            fit = smf.ols(formula, data=length_data).fit()\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Length={length_value}, Var={x}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # --------- Step 8: Collect stats ---------\n",
    "        final_results.append({\n",
    "            \"Length\": length_value,\n",
    "            \"Variable\": x,\n",
    "            \"R_squared\": round(fit.rsquared, 3),\n",
    "            \"P_value\": round(float(fit.f_pvalue), 4),\n",
    "            \"Sample_Size\": int(len(length_data))\n",
    "        })\n",
    "\n",
    "# --------- Step 9: Print final results table ---------\n",
    "final_df = pd.DataFrame(final_results)\n",
    "if not final_df.empty:\n",
    "    final_df[\"Variable\"] = pd.Categorical(final_df[\"Variable\"], categories=x_columns, ordered=True)\n",
    "    final_df = final_df.sort_values(by=[\"Length\", \"Variable\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\næœ€ç»ˆçš„å›žå½’ç»“æžœè¡¨:\")\n",
    "    print(final_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo models were fitted (empty results).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4855e",
   "metadata": {},
   "source": [
    "## Hierarchical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ebc6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical regression for LDT (single-character)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ====== 1. Load dataset ======\n",
    "file_path = \"27624_expression_7_cleaned_filtered.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Define dependent variable and hierarchical predictors\n",
    "target_col = \"zRT_LDT\"\n",
    "steps = {\n",
    "    \"Step1\": [\"NoS\", \"SUBTLEX_logCHR_CD\", \"NoWF\", \"NoM\", \"NoP\"],   # Basic lexical features\n",
    "    \"Step2\": [\"AoA_Liu\"],                                           # Age of Acquisition\n",
    "    \"Step3\": [\"Human_FAM_Liu_log\"],                                 # Human familiarity\n",
    "    \"Step4\": [\"GPT_FAM_probs_log\"]                                  # GPT familiarity\n",
    "}\n",
    "\n",
    "# ====== 2. Data cleaning ======\n",
    "# Keep only the necessary columns\n",
    "cols_needed = [target_col] + [v for lst in steps.values() for v in lst]\n",
    "df = df[cols_needed].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna()\n",
    "print(f\"Valid sample size: N = {len(df)}\")\n",
    "\n",
    "# ====== 3. Standardization (for standardized Î² coefficients) ======\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "y = df_scaled[target_col]\n",
    "results = []\n",
    "prev_r2 = 0\n",
    "\n",
    "# ====== 4. Hierarchical regression loop ======\n",
    "for i, (step_name, vars_list) in enumerate(steps.items(), start=1):\n",
    "    # Include all variables up to the current step\n",
    "    all_vars = [v for step in list(steps.values())[:i] for v in step]\n",
    "    X = df_scaled[all_vars]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    r2 = model.rsquared\n",
    "    delta_r2 = r2 - prev_r2 if i > 1 else r2\n",
    "    prev_r2 = r2\n",
    "    \n",
    "    print(f\"\\n===== {step_name} =====\")\n",
    "    print(f\"RÂ² = {r2:.3f} | Î”RÂ² = {delta_r2:.3f}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Store summary results\n",
    "    for var in X.columns:\n",
    "        if var == \"const\":\n",
    "            continue\n",
    "        results.append({\n",
    "            \"Step\": step_name,\n",
    "            \"Variable\": var,\n",
    "            \"Beta\": round(model.params[var], 3),\n",
    "            \"SE\": round(model.bse[var], 3),\n",
    "            \"t\": round(model.tvalues[var], 3),\n",
    "            \"p\": round(model.pvalues[var], 4),\n",
    "            \"R2\": round(r2, 3),\n",
    "            \"Î”R2\": round(delta_r2, 3)\n",
    "        })\n",
    "\n",
    "# ====== 5. Combine and export results ======\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\nðŸ“Š Hierarchical Regression Summary:\")\n",
    "print(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d98054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical regression for Naming (single-character)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ====== 1. Load dataset ======\n",
    "file_path = \"27624_expression_7_cleaned.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Define dependent variable and hierarchical predictors\n",
    "target_col = \"zRT_Nam_Liu\"\n",
    "steps = {\n",
    "    \"Step1\": [\"NoS\", \"SUBTLEX_logCHR_CD\", \"NoWF\", \"NoM\", \"NoP\"],   # Basic lexical features\n",
    "    \"Step2\": [\"AoA_Liu\"],                                           # Age of Acquisition\n",
    "    \"Step3\": [\"Human_FAM_Liu_log\"],                                 # Human familiarity\n",
    "    \"Step4\": [\"GPT_FAM_probs_log\"]                                  # GPT familiarity\n",
    "}\n",
    "\n",
    "# ====== 2. Data cleaning ======\n",
    "# Keep only the necessary columns\n",
    "cols_needed = [target_col] + [v for lst in steps.values() for v in lst]\n",
    "df = df[cols_needed].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna()\n",
    "print(f\"Valid sample size: N = {len(df)}\")\n",
    "\n",
    "# ====== 3. Standardization (for standardized Î² coefficients) ======\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "y = df_scaled[target_col]\n",
    "results = []\n",
    "prev_r2 = 0\n",
    "\n",
    "# ====== 4. Hierarchical regression loop ======\n",
    "for i, (step_name, vars_list) in enumerate(steps.items(), start=1):\n",
    "    # Include all variables up to the current step\n",
    "    all_vars = [v for step in list(steps.values())[:i] for v in step]\n",
    "    X = df_scaled[all_vars]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    r2 = model.rsquared\n",
    "    delta_r2 = r2 - prev_r2 if i > 1 else r2\n",
    "    prev_r2 = r2\n",
    "    \n",
    "    print(f\"\\n===== {step_name} =====\")\n",
    "    print(f\"RÂ² = {r2:.3f} | Î”RÂ² = {delta_r2:.3f}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Store summary results\n",
    "    for var in X.columns:\n",
    "        if var == \"const\":\n",
    "            continue\n",
    "        results.append({\n",
    "            \"Step\": step_name,\n",
    "            \"Variable\": var,\n",
    "            \"Beta\": round(model.params[var], 3),\n",
    "            \"SE\": round(model.bse[var], 3),\n",
    "            \"t\": round(model.tvalues[var], 3),\n",
    "            \"p\": round(model.pvalues[var], 4),\n",
    "            \"R2\": round(r2, 3),\n",
    "            \"Î”R2\": round(delta_r2, 3)\n",
    "        })\n",
    "\n",
    "# ====== 5. Combine and export results ======\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\nðŸ“Š Hierarchical Regression Summary:\")\n",
    "print(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aebc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hierarchical regression for LDT (multi-character)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ====== 1. Load dataset ======\n",
    "file_path = \"27624_word_7_cleaned_filtered.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Define dependent variable and hierarchical predictors\n",
    "target_col = \"zRT_LDT\"\n",
    "steps = {\n",
    "    \"Step1\": [\"Length\",\"NoS\", \"CF\",\"SUBTLEX_logW_CD\", \"NoWF\", \"NoM\", \"NoP\"],   # Basic lexical features\n",
    "    \"Step2\": [\"AoA\"],                                           # Age of Acquisition\n",
    "    \"Step3\": [\"Human_FAM_M_Su_log\"],                                 # Human familiarity\n",
    "    \"Step4\": [\"GPT_FAM_probs_log\"],                                  # GPT familiarity\n",
    "    \"Step5\": [\"GPT_FAM_probs_head_log\"]                                  # GPT head familiarity\n",
    "}\n",
    "\n",
    "# ====== 2. Data cleaning ======\n",
    "# Keep only the necessary columns\n",
    "cols_needed = [target_col] + [v for lst in steps.values() for v in lst]\n",
    "df = df[cols_needed].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna()\n",
    "print(f\"Valid sample size: N = {len(df)}\")\n",
    "\n",
    "# ====== 3. Standardization (for standardized Î² coefficients) ======\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "y = df_scaled[target_col]\n",
    "results = []\n",
    "prev_r2 = 0\n",
    "\n",
    "# ====== 4. Hierarchical regression loop ======\n",
    "for i, (step_name, vars_list) in enumerate(steps.items(), start=1):\n",
    "    # Include all variables up to the current step\n",
    "    all_vars = [v for step in list(steps.values())[:i] for v in step]\n",
    "    X = df_scaled[all_vars]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    r2 = model.rsquared\n",
    "    delta_r2 = r2 - prev_r2 if i > 1 else r2\n",
    "    prev_r2 = r2\n",
    "    \n",
    "    print(f\"\\n===== {step_name} =====\")\n",
    "    print(f\"RÂ² = {r2:.3f} | Î”RÂ² = {delta_r2:.3f}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Store summary results\n",
    "    for var in X.columns:\n",
    "        if var == \"const\":\n",
    "            continue\n",
    "        results.append({\n",
    "            \"Step\": step_name,\n",
    "            \"Variable\": var,\n",
    "            \"Beta\": round(model.params[var], 3),\n",
    "            \"SE\": round(model.bse[var], 3),\n",
    "            \"t\": round(model.tvalues[var], 3),\n",
    "            \"p\": round(model.pvalues[var], 4),\n",
    "            \"R2\": round(r2, 3),\n",
    "            \"Î”R2\": round(delta_r2, 3)\n",
    "        })\n",
    "\n",
    "# ====== 5. Combine and export results ======\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\nðŸ“Š Hierarchical Regression Summary:\")\n",
    "print(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hierarchical regression for Naming (multi-character)(Zhang et al.,2023)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ====== 1. Load dataset ======\n",
    "file_path = \"27624_word_7_cleaned_filtered_nam.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Define dependent variable and hierarchical predictors\n",
    "target_col = \"zRT_Nam_Zhang\"\n",
    "steps = {\n",
    "    \"Step1\": [\"Length\",\"NoS\", \"CF\",\"SUBTLEX_logW_CD\", \"NoWF\", \"NoM\", \"NoP\"],   # Basic lexical features\n",
    "    \"Step2\": [\"AoA\"],                                           # Age of Acquisition\n",
    "    \"Step3\": [\"Human_FAM_M_Su_log\"],                                 # Human familiarity\n",
    "    \"Step4\": [\"GPT_FAM_probs_log\"],                                  # GPT familiarity\n",
    "    \"Step5\": [\"GPT_FAM_probs_head_log\"]                                  # GPT head familiarity\n",
    "}\n",
    "\n",
    "# ====== 2. Data cleaning ======\n",
    "# Keep only the necessary columns\n",
    "cols_needed = [target_col] + [v for lst in steps.values() for v in lst]\n",
    "df = df[cols_needed].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna()\n",
    "print(f\"Valid sample size: N = {len(df)}\")\n",
    "\n",
    "# ====== 3. Standardization (for standardized Î² coefficients) ======\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "y = df_scaled[target_col]\n",
    "results = []\n",
    "prev_r2 = 0\n",
    "\n",
    "# ====== 4. Hierarchical regression loop ======\n",
    "for i, (step_name, vars_list) in enumerate(steps.items(), start=1):\n",
    "    # Include all variables up to the current step\n",
    "    all_vars = [v for step in list(steps.values())[:i] for v in step]\n",
    "    X = df_scaled[all_vars]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    r2 = model.rsquared\n",
    "    delta_r2 = r2 - prev_r2 if i > 1 else r2\n",
    "    prev_r2 = r2\n",
    "    \n",
    "    print(f\"\\n===== {step_name} =====\")\n",
    "    print(f\"RÂ² = {r2:.3f} | Î”RÂ² = {delta_r2:.3f}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Store summary results\n",
    "    for var in X.columns:\n",
    "        if var == \"const\":\n",
    "            continue\n",
    "        results.append({\n",
    "            \"Step\": step_name,\n",
    "            \"Variable\": var,\n",
    "            \"Beta\": round(model.params[var], 3),\n",
    "            \"SE\": round(model.bse[var], 3),\n",
    "            \"t\": round(model.tvalues[var], 3),\n",
    "            \"p\": round(model.pvalues[var], 4),\n",
    "            \"R2\": round(r2, 3),\n",
    "            \"Î”R2\": round(delta_r2, 3)\n",
    "        })\n",
    "\n",
    "# ====== 5. Combine and export results ======\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\nðŸ“Š Hierarchical Regression Summary:\")\n",
    "print(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hierarchical regression for Naming (multi-character)(Hendrix et al.,2020)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ====== 1. Load dataset ======\n",
    "file_path = \"27624_word_7_cleaned.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Define dependent variable and hierarchical predictors\n",
    "target_col = \"zRT_Nam_Hendrix\"\n",
    "steps = {\n",
    "    \"Step1\": [\"Length\",\"NoS\", \"CF\",\"SUBTLEX_logW_CD\", \"NoWF\", \"NoM\", \"NoP\"],   # Basic lexical features\n",
    "    \"Step2\": [\"AoA\"],                                           # Age of Acquisition\n",
    "    \"Step3\": [\"Human_FAM_M_Su_log\"],                                 # Human familiarity\n",
    "    \"Step4\": [\"GPT_FAM_probs_log\"],                                  # GPT familiarity\n",
    "    \"Step5\": [\"GPT_FAM_probs_head_log\"]                                  # GPT head familiarity\n",
    "}\n",
    "\n",
    "# ====== 2. Data cleaning ======\n",
    "# Keep only the necessary columns\n",
    "cols_needed = [target_col] + [v for lst in steps.values() for v in lst]\n",
    "df = df[cols_needed].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna()\n",
    "print(f\"Valid sample size: N = {len(df)}\")\n",
    "\n",
    "# ====== 3. Standardization (for standardized Î² coefficients) ======\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "y = df_scaled[target_col]\n",
    "results = []\n",
    "prev_r2 = 0\n",
    "\n",
    "# ====== 4. Hierarchical regression loop ======\n",
    "for i, (step_name, vars_list) in enumerate(steps.items(), start=1):\n",
    "    # Include all variables up to the current step\n",
    "    all_vars = [v for step in list(steps.values())[:i] for v in step]\n",
    "    X = df_scaled[all_vars]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit()\n",
    "    r2 = model.rsquared\n",
    "    delta_r2 = r2 - prev_r2 if i > 1 else r2\n",
    "    prev_r2 = r2\n",
    "    \n",
    "    print(f\"\\n===== {step_name} =====\")\n",
    "    print(f\"RÂ² = {r2:.3f} | Î”RÂ² = {delta_r2:.3f}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Store summary results\n",
    "    for var in X.columns:\n",
    "        if var == \"const\":\n",
    "            continue\n",
    "        results.append({\n",
    "            \"Step\": step_name,\n",
    "            \"Variable\": var,\n",
    "            \"Beta\": round(model.params[var], 3),\n",
    "            \"SE\": round(model.bse[var], 3),\n",
    "            \"t\": round(model.tvalues[var], 3),\n",
    "            \"p\": round(model.pvalues[var], 4),\n",
    "            \"R2\": round(r2, 3),\n",
    "            \"Î”R2\": round(delta_r2, 3)\n",
    "        })\n",
    "\n",
    "# ====== 5. Combine and export results ======\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\nðŸ“Š Hierarchical Regression Summary:\")\n",
    "print(res_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
