{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "694ba110",
   "metadata": {},
   "source": [
    "# Familiarity Rating Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bae44b",
   "metadata": {},
   "source": [
    "### GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc65c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages\n",
    "import numpy as np\n",
    "import random\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "# Experiment Settings\n",
    "RepeatTime = 1 # number of times each cue is answered by chatGPT\n",
    "AnswerTime = 780000 # total number of responses to request\n",
    "WordOnce = 1 # number of cues sent per request\n",
    "\n",
    "# OpenAI Parameters\n",
    "MyAPI = \"sk-xxxxxx\" # OpenAI API key\n",
    "MyModel = \"gpt-4o-2024-08-06\" # model to use\n",
    "MyTemperature = 0 # sampling temperature (0-2); lower -> less diverse outputs\n",
    "MyMaxTokens = 100 # max tokens per request (includes prompt + response)\n",
    "MyFreqPenalty = 0 # frequency penalty (0-1); higher -> reduces repetition\n",
    "\n",
    "client = OpenAI(api_key = MyAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input the cue words\n",
    "with open(\"./Stimulus_27624.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    data = file.read()  # read entire file content into a string\n",
    "\n",
    "    # split by newline and remove empty lines to create the cue list\n",
    "    CUES = [i for i in data.split(\"\\n\") if i != \"\"] \n",
    "\n",
    "print(len(CUES))\n",
    "print(CUES[0:WordOnce])  # show the first few cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b7170",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomize items in the CUES\n",
    "CUES = [j for i in [random.sample(CUES, len(CUES)) for i in range(RepeatTime)] for j in i] \n",
    "# Randomly shuffle CUES RepeatTime times to create a concatenated list repeated RepeatTime times\n",
    "\n",
    "CUES_all = CUES\n",
    "\n",
    "print(len(CUES_all))\n",
    "print(CUES_all[0:3]) # Show the first 3 items of CUES_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f399ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, warnings, pickle, csv\n",
    "from openai import OpenAI\n",
    "\n",
    "class Gpta:\n",
    "\n",
    "    def __init__(self, data, log, results):\n",
    "        self.log = log\n",
    "        self.results = results\n",
    "        self.data = data\n",
    "        self.cue = []\n",
    "        self.temp = MyTemperature\n",
    "        self.maxtokens = MyMaxTokens\n",
    "        self.freqpenalty = MyFreqPenalty\n",
    "        self.client = OpenAI(api_key=MyAPI)\n",
    "\n",
    "    def gpt_associations(self, cue, model=MyModel):\n",
    "        # Build the message list for the API request\n",
    "        message = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Complete the following tasks as a native speaker of Simplified Chinese: Familiarity is a measure of how familiar something is. \"\n",
    "                    + \"A Chinese word is very FAMILIAR if you see/hear it often and it is easily recognisable.\"\n",
    "                    + \"In contrast, a Chinese word is very UNFAMILIAR if you rarely see/hear it and it is relatively unrecognisable. \"\n",
    "                    + \"Please indicate how familiar you think each Chinese word is on a scale from 1 (VERY UNFAMILIAR) to 7 (VERY FAMILIAR), with the midpoint representing moderate familiarity. \"\n",
    "                    + \"The Chinese word is: \"\n",
    "                    + cue\n",
    "                    + \" Only answer a number from 1 to 7. Please limit your answer to numbers.\"\n",
    "                ),\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Send the chat completion request to the OpenAI model\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=message,\n",
    "            temperature=self.temp,\n",
    "            max_tokens=self.maxtokens,\n",
    "            frequency_penalty=self.freqpenalty,\n",
    "            logprobs=True,  # return log probabilities for output tokens\n",
    "            top_logprobs=7,  # return top logprobs for each token\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        self.cue.append(cue)\n",
    "\n",
    "        # Append cue and model response to the log file\n",
    "        with open(self.log, \"a\", encoding=\"utf-8\") as file:\n",
    "            # Write the cue\n",
    "            file.write(cue + \"，，，\\n\")\n",
    "            # Write the model's response\n",
    "            file.write(response.choices[0].message.content + \"。。。\\n\")\n",
    "\n",
    "            # Extract logprobs information for each token\n",
    "            logprobs_content = response.choices[0].logprobs.content  # get logprobs content\n",
    "\n",
    "            # Iterate tokens to format top_logprobs entries\n",
    "            top_logprobs_str = \"\"\n",
    "            for token_info in logprobs_content:\n",
    "                top_logprobs = token_info.top_logprobs\n",
    "\n",
    "                # Convert top_logprobs to simple 'token: logprob' string format\n",
    "                top_logprobs_entry = \", \".join(\n",
    "                    [f\"'{top_prob.token}': {top_prob.logprob}\" for top_prob in top_logprobs]\n",
    "                )\n",
    "                # Accumulate top_logprobs lines into a single string\n",
    "                top_logprobs_str += f\"{top_logprobs_entry}\\n\"\n",
    "\n",
    "            # Write the accumulated top_logprobs to the log file\n",
    "            file.write(top_logprobs_str)\n",
    "\n",
    "        # Append the result dict to the results list\n",
    "        self.results.append(\n",
    "            {\n",
    "                \"cue\": cue,\n",
    "                \"model\": model,\n",
    "                \"message\": message,\n",
    "                \"response\": response,\n",
    "                \"temperature\": self.temp,\n",
    "                \"max_tokens\": self.maxtokens,\n",
    "                \"frequency_penalty\": self.freqpenalty,\n",
    "                \"top_logprobs_details\": logprobs_content,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Save the current results list to a pickle file\n",
    "        with open(self.data, \"wb\") as file:\n",
    "            pickle.dump(self.results, file)\n",
    "\n",
    "        print(\"获取并保存线索成功：\" + cue)\n",
    "        print(\"chatGPT的回答是：\" + response.choices[0].message.content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7222e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpta = Gpta(\n",
    "    data = \"GPT_familiar_results_27624_expression_7.pkl\",\n",
    "    log = \"GPT_familiar_results_27624_expression_7.txt\",\n",
    "    results=[],\n",
    ")\n",
    "\n",
    "# Process unique cues and collect responses\n",
    "k = 0\n",
    "for i in CUES_all: # iterate through each cue\n",
    "    gpta.gpt_associations(i) # call API to get and save response\n",
    "    k += 1\n",
    "    if k >= AnswerTime: # stop after AnswerTime responses\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014c464",
   "metadata": {},
   "source": [
    "### Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import time, warnings, pickle, csv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "# Experiment Settings\n",
    "RepeatTime = 1 # number of times each cue is answered by Qwen\n",
    "AnswerTime = 1000000 # total number of responses to request\n",
    "WordOnce = 1 # number of cues sent per request\n",
    "\n",
    "# OpenAI Parameters\n",
    "\n",
    "MyAPI = \"sk-xxxxx\" # OpenAI API key\n",
    "MyModel = \"qwen-max\" # model to use\n",
    "MyTemperature = 0.7 # sampling temperature (0-2)\n",
    "MyMaxTokens = 100 # max tokens per request (includes prompt + response)\n",
    "\n",
    "client = OpenAI(api_key = MyAPI, base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\")  # client configured for compatible-mode endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input the cue words\n",
    "with open(\"./Stimulus_27624.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    CUES = [i for i in data.split(\"\\n\") if i != \"\"] \n",
    "\n",
    "print(len(CUES))\n",
    "print(CUES[0:WordOnce]) # show the count and the first WordOnce items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07313e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomize items in the CUES\n",
    "CUES = [j for i in [random.sample(CUES, len(CUES)) for i in range(RepeatTime)] for j in i] \n",
    "CUES_all = CUES\n",
    "\n",
    "print(len(CUES_all))\n",
    "print(CUES_all[0:3]) # Print total count and the first 3 items of CUES_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e1dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set functions getting access to qwen\n",
    "\n",
    "class Gpta: \n",
    "\n",
    "    def __init__(self, data, log, results): \n",
    "        self.log = log\n",
    "        self.results = results\n",
    "        self.data = data\n",
    "        self.cue = [] # list to track processed cues\n",
    "        self.temp = MyTemperature  # sampling temperature (from notebook settings)\n",
    "        self.maxtokens = MyMaxTokens  # max tokens per request (from notebook settings)\n",
    "        self.client = OpenAI(api_key = MyAPI)  # OpenAI client created with API key\n",
    "\n",
    "    def gpt_associations(self, cue, model = MyModel): \n",
    "        # Send a single cue to the model and save the response.\n",
    "        # Build the user prompt (in Chinese) that asks for a familiarity rating 1-7.\n",
    "        message = [\n",
    "            {\n",
    "                \"role\": \"user\", # role: user message to the model\n",
    "                \"content\": \"作为一个简体中文母语者完成以下任务：熟悉度是衡量某个东西对你来说有多熟悉的测量标准。\"\n",
    "                + \"如果一个中文词或者汉字是你经常看到或听到的，并且很容易认出来，那么它就是非常熟悉的。\"\n",
    "                + \"相反，如果一个中文词或者汉字是你很少看到或听到的，并且不太容易认出来，那么它就是非常不熟悉的。\"\n",
    "                + \"请在1（非常不熟悉）到7（非常熟悉）的范围内，评估每个中文词或者汉字在你看来有多熟悉，其中的中点代表适中的熟悉度。\"\n",
    "                + \"这一个中文词或者汉字是：\"\n",
    "                + cue\n",
    "                + \"请只回答一个从1到7的数字，并确保答案仅限于数字。\", # user prompt content\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Call the chat completion endpoint\n",
    "        response = client.chat.completions.create(\n",
    "            model = model,\n",
    "            messages = message,\n",
    "            temperature = self.temp,\n",
    "            max_tokens = self.maxtokens,\n",
    "            stream = False,\n",
    "        )\n",
    "\n",
    "        self.cue.append(cue) # record the cue\n",
    "\n",
    "        with open(self.log, \"a\", encoding = \"utf-8\") as file: # append cue and model reply to log file\n",
    "            # Write cue and model text to the log\n",
    "            file.write(\n",
    "                cue\n",
    "                + \"，，，\\n\"\n",
    "                + response.choices[0].message.content\n",
    "                + \"。。。\"\n",
    "                + \"\\n\"\n",
    "            )\n",
    "\n",
    "        # Append a result dictionary to the in-memory results list\n",
    "        self.results.append(\n",
    "            {\n",
    "                \"cue\": cue,\n",
    "                \"model\": model,\n",
    "                \"message\": message,\n",
    "                \"response\": response,\n",
    "                \"temperature\": self.temp,\n",
    "                \"max_tokens\": self.maxtokens,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Persist results list to a pickle file\n",
    "        with open(self.data, \"wb\") as file:\n",
    "            pickle.dump(self.results, file)\n",
    "\n",
    "        # Print confirmations\n",
    "        print(\"获取并保存线索成功：\" + cue)\n",
    "        print(\"qwen的回答是：\" + response.choices[0].message.content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpta = Gpta(\n",
    "    data = \"qwen_familiar_results.pkl\",\n",
    "    log = \"qwen_familiar_results.txt\",\n",
    "    results=[],\n",
    ")\n",
    "\n",
    "# Iterate through cues and request ratings; stop after AnswerTime\n",
    "k = 0\n",
    "for cue in CUES_all:  \n",
    "    try:\n",
    "        # Send request and count successful responses\n",
    "        gpta.gpt_associations(cue)  \n",
    "        k += 1  \n",
    "\n",
    "        if k >= AnswerTime: \n",
    "            break\n",
    "\n",
    "        # brief pause between requests to avoid rate limits (0.3s)\n",
    "        time.sleep(0.3) \n",
    "    except Exception as e:\n",
    "        # log exceptions and continue\n",
    "        print(f\"发生错误：{e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
